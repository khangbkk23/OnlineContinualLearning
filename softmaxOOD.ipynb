{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accc8cc7",
   "metadata": {},
   "source": [
    "## Import libraries                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921bc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, Optional, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643be605",
   "metadata": {},
   "source": [
    "## SoftmaxOOD Class\n",
    "Enhanced softmax-based Out-of-Distribution detection with multiple uncertainty estimators.\n",
    "* Compatible with the existing SoftmaxDetector interface while providing additional capabilities.\n",
    "    \n",
    "* Based on the theoretical analysis of softmax uncertainty estimation from the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add48ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "class SoftmaxOOD:\n",
    "    def __init__(self, method=\"umax\"):\n",
    "        self.method = method\n",
    "        self.tau = None\n",
    "        self.gmm = None\n",
    "        self.training_stats = {}\n",
    "        self.num_classes = None\n",
    "        self.final_layer_weights = None\n",
    "        \n",
    "    def _get_actual_model(self, model):\n",
    "        \"\"\"Extract the actual model from wrapped models like OnlineEWC.\"\"\"\n",
    "        if hasattr(model, 'model'):\n",
    "            return model.model\n",
    "        return model\n",
    "    \n",
    "    def _get_device_from_model(self, model):\n",
    "        \"\"\"Get device from model parameters.\"\"\"\n",
    "        actual_model = self._get_actual_model(model)\n",
    "        return next(actual_model.parameters()).device\n",
    "        \n",
    "    def fit(self, model, buffer, device=None, num_classes=None, reg_cov=1e-5):\n",
    "        \"\"\"\n",
    "        Fit the detector parameters using training data.\n",
    "        \n",
    "        Args:\n",
    "            model: Neural network model (can be wrapped like OnlineEWC)\n",
    "            buffer: Data buffer containing training samples\n",
    "            device: Computing device (auto-detected if None)\n",
    "            num_classes: Number of classes in the dataset\n",
    "            reg_cov: Regularization for covariance (used for density estimation)\n",
    "        \"\"\"\n",
    "        # Auto-detect device if not provided\n",
    "        if device is None:\n",
    "            device = self._get_device_from_model(model)\n",
    "            \n",
    "        self.num_classes = num_classes\n",
    "        actual_model = self._get_actual_model(model)\n",
    "        \n",
    "        if self.method == 'udensity':\n",
    "            X_train, y_train = buffer.get_all_data()\n",
    "            if X_train is None:\n",
    "                return\n",
    "            \n",
    "            actual_model.eval()\n",
    "            features_list = []\n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(X_train), 256):\n",
    "                    batch = X_train[i:i+256].to(device).float()\n",
    "                    _, features = actual_model(batch)\n",
    "                    features_list.append(features.cpu().numpy())\n",
    "            all_features = np.concatenate(features_list, axis=0)\n",
    "            \n",
    "            # Use provided num_classes or determine from labels\n",
    "            if y_train is not None:\n",
    "                actual_classes = len(torch.unique(y_train))\n",
    "                if num_classes is not None:\n",
    "                    self.num_classes = min(num_classes, actual_classes)\n",
    "                else:\n",
    "                    self.num_classes = actual_classes\n",
    "            \n",
    "            self.gmm = GaussianMixture(\n",
    "                n_components=self.num_classes,\n",
    "                covariance_type='full',\n",
    "                reg_covar=reg_cov,\n",
    "                random_state=42\n",
    "            )\n",
    "            self.gmm.fit(all_features)\n",
    "        \n",
    "        # Extract final layer weights for mental model and analysis\n",
    "        self._extract_final_layer_weights(actual_model)\n",
    "                \n",
    "    def _extract_final_layer_weights(self, model):\n",
    "        \"\"\"Extract weights from the final linear layer.\"\"\"\n",
    "        final_layer = None\n",
    "        for module in reversed(list(model.modules())):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                final_layer = module\n",
    "                break\n",
    "        \n",
    "        if final_layer is not None:\n",
    "            self.final_layer_weights = final_layer.weight.data.T.detach()\n",
    "            \n",
    "    def score(self, model, x, device=None):\n",
    "        \"\"\"\n",
    "        Compute uncertainty scores for input samples.\n",
    "        Higher scores indicate higher uncertainty (more likely OOD).\n",
    "        \"\"\"\n",
    "        # Auto-detect device if not provided\n",
    "        if device is None:\n",
    "            device = self._get_device_from_model(model)\n",
    "            \n",
    "        actual_model = self._get_actual_model(model)\n",
    "        actual_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, features = actual_model(x)  # Model returns (logits, features)\n",
    "            \n",
    "        if self.method == 'umax':\n",
    "            return self._compute_umax(logits)\n",
    "        elif self.method == 'uentropy':\n",
    "            return self._compute_uentropy(logits)\n",
    "        elif self.method == 'udensity':\n",
    "            return self._compute_udensity(features)\n",
    "        elif self.method == 'mental_model':\n",
    "            return self._compute_mental_model(logits, features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "    \n",
    "    def _compute_umax(self, logits):\n",
    "        \"\"\"Compute Umax: negative maximum predicted probability.\"\"\"\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        max_probs = probs.max(dim=1)[0]\n",
    "        return -max_probs  # Higher (less negative) = more uncertain\n",
    "    \n",
    "    def _compute_uentropy(self, logits):\n",
    "        \"\"\"Compute Uentropy: prediction entropy.\"\"\"\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "        return entropy  # Higher = more uncertain\n",
    "    \n",
    "    def _compute_udensity(self, features):\n",
    "        \"\"\"Compute Udensity: negative log-likelihood under fitted GMM.\"\"\"\n",
    "        if self.gmm is None:\n",
    "            raise ValueError(\"Must call fit() first for density-based detection\")\n",
    "        \n",
    "        features_np = features.cpu().numpy()\n",
    "        log_likelihood = self.gmm.score_samples(features_np)\n",
    "        return torch.tensor(-log_likelihood, device=features.device)\n",
    "    \n",
    "    def _compute_mental_model(self, logits, features):\n",
    "        \"\"\"\n",
    "        Compute uncertainty using the complete mental model from Proposition 6.\n",
    "        U_max_mental(z) = -1 / (1 + (K-1) * exp(-||z|| * (1/(K-1) + max cos θ_i,z)))\n",
    "        \"\"\"\n",
    "        if self.num_classes is None:\n",
    "            self.num_classes = logits.size(1)\n",
    "        \n",
    "        # Compute feature norms ||z||\n",
    "        feature_norms = torch.norm(features, dim=1)\n",
    "        \n",
    "        # Compute max cosine similarity with weight vectors\n",
    "        if self.final_layer_weights is not None:\n",
    "            max_cos_theta = self._compute_max_cosine_alignment(features, self.final_layer_weights)\n",
    "        else:\n",
    "            # Fallback: approximate using softmax probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            max_cos_theta = torch.max(probs, dim=1)[0]\n",
    "        \n",
    "        # Mental model formula from equation (86)\n",
    "        K = self.num_classes\n",
    "        exponent = -feature_norms * (1.0/(K - 1) + max_cos_theta)\n",
    "        uncertainty = -1.0 / (1.0 + (K - 1) * torch.exp(exponent))\n",
    "        \n",
    "        return uncertainty\n",
    "    \n",
    "    def _compute_max_cosine_alignment(self, features, weights):\n",
    "        \"\"\"\n",
    "        Compute max_i cos(θ_{z,i}) - maximum cosine similarity between \n",
    "        feature vectors and weight vectors.\n",
    "        \"\"\"\n",
    "        # Normalize features and weights\n",
    "        features_norm = F.normalize(features, dim=1)\n",
    "        weights_norm = F.normalize(weights, dim=0)  # weights is (H, K)\n",
    "        \n",
    "        # Compute cosine similarities: (N, K)\n",
    "        cosine_similarities = torch.mm(features_norm, weights_norm)\n",
    "        \n",
    "        # Return maximum similarity for each feature vector\n",
    "        max_cosines = torch.max(cosine_similarities, dim=1)[0]\n",
    "        \n",
    "        return max_cosines\n",
    "    \n",
    "    def compute_feature_statistics(self, features):\n",
    "        \"\"\"\n",
    "        Compute feature statistics as described in Section 5.\n",
    "        Returns ||z|| and max cos θ statistics.\n",
    "        \"\"\"\n",
    "        feature_norms = torch.norm(features, dim=1)\n",
    "        \n",
    "        stats = {\n",
    "            'feature_norm_mean': torch.mean(feature_norms).item(),\n",
    "            'feature_norm_std': torch.std(feature_norms).item(),\n",
    "            'feature_norm_min': torch.min(feature_norms).item(),\n",
    "            'feature_norm_max': torch.max(feature_norms).item()\n",
    "        }\n",
    "        \n",
    "        if self.final_layer_weights is not None:\n",
    "            max_cosines = self._compute_max_cosine_alignment(features, self.final_layer_weights)\n",
    "            stats.update({\n",
    "                'max_cosine_mean': torch.mean(max_cosines).item(),\n",
    "                'max_cosine_std': torch.std(max_cosines).item(),\n",
    "                'max_cosine_min': torch.min(max_cosines).item(),\n",
    "                'max_cosine_max': torch.max(max_cosines).item()\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def detect(self, model, x, device=None):\n",
    "        \"\"\"Binary OOD detection using the fitted threshold.\"\"\"\n",
    "        if self.tau is None:\n",
    "            raise ValueError(\"Must call set_threshold() first\")\n",
    "        return self.score(model, x, device) > self.tau\n",
    "    \n",
    "    def set_threshold(self, model, buffer, device=None, false_positive_rate=0.2):\n",
    "        \"\"\"Set detection threshold based on in-distribution data.\"\"\"\n",
    "        # Auto-detect device if not provided\n",
    "        if device is None:\n",
    "            device = self._get_device_from_model(model)\n",
    "            \n",
    "        X_id, _ = buffer.get_all_data()\n",
    "        if X_id is None:\n",
    "            return\n",
    "            \n",
    "        s_id = []\n",
    "        for i in range(0, len(X_id), 256):\n",
    "            batch = X_id[i:i+256].to(device).float()\n",
    "            s_id.append(self.score(model, batch, device))\n",
    "        s_id = torch.cat(s_id)\n",
    "        \n",
    "        self.tau = torch.quantile(s_id, 1 - false_positive_rate).item()\n",
    "    \n",
    "    def analyze_model_structure(self, model, device=None):\n",
    "        \"\"\"\n",
    "        Analyze the decision boundary structure of the model's final layer.\n",
    "        \"\"\"\n",
    "        # Auto-detect device if not provided\n",
    "        if device is None:\n",
    "            device = self._get_device_from_model(model)\n",
    "            \n",
    "        actual_model = self._get_actual_model(model)\n",
    "        actual_model.eval()\n",
    "        \n",
    "        # Get the final layer weights\n",
    "        final_layer = None\n",
    "        for module in reversed(list(actual_model.modules())):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                final_layer = module\n",
    "                break\n",
    "        \n",
    "        if final_layer is None:\n",
    "            return {\"error\": \"Could not find final linear layer\"}\n",
    "        \n",
    "        weights = final_layer.weight.data  # Shape: (num_classes, feature_dim)\n",
    "        biases = final_layer.bias.data if final_layer.bias is not None else None\n",
    "        \n",
    "        # Transpose to match paper notation (H x K)\n",
    "        weights = weights.T\n",
    "        \n",
    "        return self._analyze_decision_boundary_structure(weights, biases)\n",
    "    \n",
    "    def _analyze_decision_boundary_structure(self, weights, biases=None):\n",
    "        \"\"\"\n",
    "        Analyze decision boundary structure following Section 4.2.\n",
    "        Checks for optimal structure properties:\n",
    "        1. ||w_i|| = constant (equal weight magnitudes)\n",
    "        2. cos θ_{i,j} = -1/(K-1) (evenly distributed weights)\n",
    "        3. Bias values ≈ 0\n",
    "        \"\"\"\n",
    "        weights_np = weights.detach().cpu().numpy()\n",
    "        num_classes = weights_np.shape[1]\n",
    "        \n",
    "        # 1. Compute weight magnitudes\n",
    "        weight_norms = np.linalg.norm(weights_np, axis=0)\n",
    "        \n",
    "        # 2. Compute pairwise cosine similarities\n",
    "        weights_normalized = weights_np / (weight_norms + 1e-8)\n",
    "        cosine_similarities = np.dot(weights_normalized.T, weights_normalized)\n",
    "        \n",
    "        # Remove diagonal elements for pairwise analysis\n",
    "        mask = ~np.eye(num_classes, dtype=bool)\n",
    "        pairwise_cosines = cosine_similarities[mask]\n",
    "        \n",
    "        # 3. Theoretical optimal cosine for evenly distributed weights\n",
    "        optimal_cosine = -1.0 / (num_classes - 1) if num_classes > 1 else 0.0\n",
    "        \n",
    "        results = {\n",
    "            'num_classes': num_classes,\n",
    "            'weight_norm_mean': np.mean(weight_norms),\n",
    "            'weight_norm_std': np.std(weight_norms),\n",
    "            'weight_norm_uniformity': np.std(weight_norms) / (np.mean(weight_norms) + 1e-8),\n",
    "            'pairwise_cosine_mean': np.mean(pairwise_cosines),\n",
    "            'pairwise_cosine_std': np.std(pairwise_cosines),\n",
    "            'optimal_cosine_target': optimal_cosine,\n",
    "            'cosine_deviation_from_optimal': abs(np.mean(pairwise_cosines) - optimal_cosine),\n",
    "            'structure_optimality_score': 1.0 / (1.0 + abs(np.mean(pairwise_cosines) - optimal_cosine)),\n",
    "            'is_structure_optimal': abs(np.mean(pairwise_cosines) - optimal_cosine) < 0.1\n",
    "        }\n",
    "        \n",
    "        if biases is not None:\n",
    "            biases_np = biases.detach().cpu().numpy()\n",
    "            results.update({\n",
    "                'bias_mean': np.mean(biases_np),\n",
    "                'bias_std': np.std(biases_np),\n",
    "                'bias_magnitude': np.mean(np.abs(biases_np)),\n",
    "                'biases_near_zero': np.mean(np.abs(biases_np)) < 0.1\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compute_valid_ood_region_size(self, model, device=None, sample_size=10000):\n",
    "        \"\"\"\n",
    "        Estimate the size of the valid OOD region by sampling.\n",
    "        \"\"\"\n",
    "        # Auto-detect device if not provided\n",
    "        if device is None:\n",
    "            device = self._get_device_from_model(model)\n",
    "            \n",
    "        if self.final_layer_weights is None:\n",
    "            return {\"error\": \"Need final layer weights for region analysis\"}\n",
    "        \n",
    "        # Sample random points in feature space\n",
    "        feature_dim = self.final_layer_weights.size(0)\n",
    "        sample_points = torch.randn(sample_size, feature_dim, device=device)\n",
    "        \n",
    "        # Compute uncertainty scores for sampled points\n",
    "        actual_model = self._get_actual_model(model)\n",
    "        actual_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Create dummy logits by passing through final layer\n",
    "            logits = torch.mm(sample_points, self.final_layer_weights.to(device))\n",
    "            \n",
    "        uncertainty_scores = self._compute_umax(logits)\n",
    "        \n",
    "        # Estimate region properties\n",
    "        mean_uncertainty = torch.mean(uncertainty_scores).item()\n",
    "        std_uncertainty = torch.std(uncertainty_scores).item()\n",
    "        \n",
    "        return {\n",
    "            'sample_size': sample_size,\n",
    "            'mean_uncertainty': mean_uncertainty,\n",
    "            'std_uncertainty': std_uncertainty,\n",
    "            'high_uncertainty_fraction': (uncertainty_scores > mean_uncertainty + std_uncertainty).float().mean().item()\n",
    "        }\n",
    "    \n",
    "    def get_method_info(self):\n",
    "        \"\"\"Return information about the current uncertainty method.\"\"\"\n",
    "        info = {\n",
    "            'umax': 'Maximum predicted probability (negative) - Equation (2)',\n",
    "            'uentropy': 'Prediction entropy - Equation (2)', \n",
    "            'udensity': 'Gaussian Mixture Model density - Equation (3)',\n",
    "            'mental_model': 'Mental model approximation - Proposition 6'\n",
    "        }\n",
    "        return {\n",
    "            'method': self.method,\n",
    "            'description': info.get(self.method, 'Unknown method'),\n",
    "            'requires_fitting': self.method in ['udensity'],\n",
    "            'threshold_set': self.tau is not None,\n",
    "            'num_classes': self.num_classes,\n",
    "            'has_weights': self.final_layer_weights is not None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72552823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxDetector(SoftmaxOOD):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for the original SoftmaxDetector with same interface.\n",
    "    Uses Umax method by default for backward compatibility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(method='umax')\n",
    "#* Usage:\n",
    "'''\n",
    "def enhanced_usage():\n",
    "    \"\"\"Demonstrate the enhanced capabilities.\"\"\"\n",
    "    \n",
    "    # Create detectors with different methods\n",
    "    detectors = {\n",
    "        'umax': SoftmaxOOD(method='umax'),\n",
    "        'uentropy': SoftmaxOOD(method='uentropy'), \n",
    "        'udensity': SoftmaxOOD(method='udensity'),\n",
    "        'mental_model': SoftmaxOOD(method='mental_model')\n",
    "    }\n",
    "    \n",
    "    print(\"Available uncertainty methods:\")\n",
    "    for name, detector in detectors.items():\n",
    "        info = detector.get_method_info()\n",
    "        print(f\"- {name}: {info['description']}\")\n",
    "    \n",
    "    return detectors\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
