{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c82cd1",
   "metadata": {},
   "source": [
    "# GSS (Gradient-based Subspace Search) Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ae69cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Notebook last modified at: 2025-07-31 21:09:13\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchvision.models import resnet34 as torchvision_resnet34\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Checking status of GPU and time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Notebook last modified at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9558101",
   "metadata": {},
   "source": [
    "## Replay Buffer for EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67203e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_per_class=100, model=None, loss_fn=None, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer with GSS-Greedy selection.\n",
    "        \n",
    "        Args:\n",
    "            max_per_class (int): Maximum number of exemplars per class.\n",
    "            model: Neural network model for gradient computation (e.g., ResNetSmall).\n",
    "            loss_fn: Loss function for gradient computation (e.g., CrossEntropyLoss).\n",
    "            device: Device to perform computations (cuda or cpu).\n",
    "        \"\"\"\n",
    "        self.max_per_class = max_per_class\n",
    "        self.buffer = defaultdict(list)\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn or nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self.device = device\n",
    "\n",
    "    def compute_gradient(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute normalized gradient for a single sample.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (single sample).\n",
    "            y (torch.Tensor): Label tensor (single label).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Normalized gradient vector.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        x, y = x.unsqueeze(0).to(self.device), y.unsqueeze(0).to(self.device)\n",
    "        self.model.zero_grad()\n",
    "        feats, logits = self.model(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Collect gradients\n",
    "        grad = []\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad.append(param.grad.flatten())\n",
    "        grad = torch.cat(grad)\n",
    "        grad_norm = torch.norm(grad, p=2)\n",
    "        return grad / (grad_norm + 1e-8)  # Normalize gradient\n",
    "\n",
    "    def add_examples(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Add examples to the replay buffer using GSS-Greedy selection.\n",
    "        \n",
    "        Args:\n",
    "            x_batch (torch.Tensor): Batch of input data.\n",
    "            y_batch (torch.Tensor): Corresponding labels for the input data.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.loss_fn is None:\n",
    "            raise ValueError(\"Model and loss function must be set for GSS-Greedy selection.\")\n",
    "        \n",
    "        for cls in set(y_batch.cpu().numpy()):\n",
    "            cls = int(cls)\n",
    "            # Collect samples for this class\n",
    "            cls_indices = (y_batch == cls).nonzero(as_tuple=True)[0]\n",
    "            if not cls_indices.numel():\n",
    "                continue\n",
    "            \n",
    "            cls_samples = [(x_batch[i], y_batch[i]) for i in cls_indices]\n",
    "            current_samples = self.buffer[cls] + cls_samples\n",
    "            \n",
    "            if len(current_samples) <= self.max_per_class:\n",
    "                # If within limit, keep all samples\n",
    "                self.buffer[cls] = current_samples\n",
    "            else:\n",
    "                # Compute gradients for all samples\n",
    "                gradients = []\n",
    "                samples = []\n",
    "                for x, y in current_samples:\n",
    "                    grad = self.compute_gradient(x, y)\n",
    "                    gradients.append(grad)\n",
    "                    samples.append((x, y))\n",
    "                \n",
    "                # GSS-Greedy selection\n",
    "                selected_indices = []\n",
    "                for _ in range(self.max_per_class):\n",
    "                    if not selected_indices:\n",
    "                        idx = np.random.randint(0, len(samples))\n",
    "                    else:\n",
    "                        distances = []\n",
    "                        for i, grad in enumerate(gradients):\n",
    "                            if i in selected_indices:\n",
    "                                continue\n",
    "                            min_dist = min([torch.norm(grad - gradients[j]) for j in selected_indices])\n",
    "                            distances.append((i, min_dist))\n",
    "                        idx = max(distances, key=lambda x: x[1])[0]\n",
    "                    selected_indices.append(idx)\n",
    "                \n",
    "                # Update buffer with selected samples\n",
    "                self.buffer[cls] = [samples[i] for i in selected_indices]\n",
    "\n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Get all data from the replay buffer as a TensorDataset.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars in the buffer.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls, examples in self.buffer.items():\n",
    "            if examples:\n",
    "                xs.extend([x for x, _ in examples])\n",
    "                ys.extend([y for _, y in examples])\n",
    "        if not xs:\n",
    "            return None, None\n",
    "        return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a1e8c",
   "metadata": {},
   "source": [
    "# EWC class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926585f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWC:\n",
    "    def __init__(self, model, dataloader, device, samples=500):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.params = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "        self.fisher = self._compute_fisher(dataloader, samples)\n",
    "\n",
    "    def _compute_fisher(self, dataloader, samples):\n",
    "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters()}\n",
    "        self.model.eval()\n",
    "        count = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(self.device)\n",
    "            self.model.zero_grad()\n",
    "            feats, logits = self.model(x)\n",
    "            # prob = F.softmax(logits, dim=1)\n",
    "\n",
    "            # log_prob = F.log_softmax(logits, dim=1)[range(len(y)), y].mean()\n",
    "            # log_prob.backward()\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=1)\n",
    "            # sum negative log‐likelihood over batch\n",
    "            loss_batch = -log_probs[range(len(y)), y].sum()\n",
    "            loss_batch.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                fisher[n] += p.grad.data.pow(2)\n",
    "            count += x.size(0)   # count by number of *samples*\n",
    "            if count >= samples:\n",
    "                break\n",
    "        return {n: f / count for n, f in fisher.items()}\n",
    "\n",
    "    def penalty(self, model, lambda_ewc):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n not in self.fisher: continue\n",
    "            f, p0 = self.fisher[n], self.params[n]\n",
    "            if p.shape == p0.shape:\n",
    "                loss += (f * (p - p0).pow(2)).sum()\n",
    "            else:\n",
    "                # assume this is the expanded fc.weight or fc.bias\n",
    "                # only penalize the first p0.shape[...] entries\n",
    "                if 'fc.weight' in n and p.dim()==2:\n",
    "                    loss += (f * (p[:p0.size(0)] - p0).pow(2)).sum()\n",
    "                elif 'fc.bias' in n and p.dim()==1:\n",
    "                    loss += (f * (p[:p0.size(0)] - p0).pow(2)).sum()\n",
    "        return (lambda_ewc / 2) * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd53080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerTaskEWC:\n",
    "    \"\"\"\n",
    "    Collects multiple (params, fisher) snapshots—one per past task—\n",
    "    and, at training‐time, computes the sum of all EWC penalties.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device, ewc_paths: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - model (nn.Module):  The “current” model (whose parameter names\n",
    "                                must match those stored on disk).\n",
    "          - device:            CPU / CUDA device.\n",
    "          - ewc_paths:         List of file‐paths: ['ewc_task_1.pt', 'ewc_task_2.pt', ...].\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "\n",
    "        self.past_task_params = []  # list of dict: each dict maps name→tensor (θ^{*(k)})\n",
    "        self.past_task_fishers = [] # list of dict: each dict maps name→tensor (F^{(k)})\n",
    "\n",
    "        # Load all saved EWC files:\n",
    "        for path in ewc_paths:\n",
    "            data = torch.load(path, map_location='cpu')\n",
    "            # data['params'] and data['fisher'] are both dict(name→cpu_tensor)\n",
    "            # Move them to the correct device now:\n",
    "            params_k = {name: param.to(self.device) for name, param in data['params'].items()}\n",
    "            fisher_k = {name: fisher.to(self.device) for name, fisher in data['fisher'].items()}\n",
    "            self.past_task_params.append(params_k)\n",
    "            self.past_task_fishers.append(fisher_k)\n",
    "\n",
    "\n",
    "    def penalty(self, model, lambda_ewc):\n",
    "        \"\"\"\n",
    "        Loops over each past task k, then each parameter name,\n",
    "        and accumulates F^{(k)}_i * (θ_i - θ^{*(k)}_i)^2.\n",
    "\n",
    "        Returns:  (λ/2) * [sum over tasks & params of F (θ - θ*)^2]\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Iterate over each past‐task snapshot:\n",
    "        for params_k, fisher_k in zip(self.past_task_params, self.past_task_fishers):\n",
    "            for name, param in model.named_parameters():\n",
    "                # If this parameter existed when snapshot_k was taken:\n",
    "                if name not in fisher_k:\n",
    "                    continue\n",
    "\n",
    "                θ_star = params_k[name]      # θ^{*(k)}\n",
    "                Fk      = fisher_k[name]     # F^{(k)}\n",
    "\n",
    "                if param.shape == θ_star.shape:\n",
    "                    total_loss += (Fk * (param - θ_star).pow(2)).sum()\n",
    "                else:\n",
    "                    # If some layers were expanded (e.g. classifier head grew),\n",
    "                    # only penalize the “old” slice [0:θ_star.shape[...]].\n",
    "                    if 'fc.weight' in name and param.dim() == 2:\n",
    "                        total_loss += (Fk * (param[:θ_star.size(0)] - θ_star).pow(2)).sum()\n",
    "                    elif 'fc.bias' in name and param.dim() == 1:\n",
    "                        total_loss += (Fk * (param[:θ_star.size(0)] - θ_star).pow(2)).sum()\n",
    "                    # else: if other layers changed shape unexpectely, you may skip them.\n",
    "\n",
    "        # Multiply by λ/2:\n",
    "        return (lambda_ewc / 2) * total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-venv)",
   "language": "python",
   "name": "pytorch-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
