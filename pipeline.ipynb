{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fba96a4",
   "metadata": {},
   "source": [
    "We will follow the structure:\n",
    "[Data Input/Stream]\n",
    "       ↓\n",
    "[Out-of-Distribution Detection]\n",
    "       ↓\n",
    "[Task Boundary Detector/Scheduler] → [Replay Buffer]\n",
    "       ↓                                 ↑↓\n",
    "[Neural Network Model] ←──────────────────┘\n",
    "       ↓\n",
    "[Regularization Mechanism]\n",
    "       ↓\n",
    "[Loss Function]\n",
    "       ↓\n",
    "[Training Loop/Optimizer]\n",
    "       ↓\n",
    "[Evaluation Module]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0d1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f9410",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9750fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class ImprovedTaskSplitter:\n",
    "    \"\"\"\n",
    "    Improved task splitting strategies for Online Continual Learning\n",
    "    that considers feature similarity and task difficulty balance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, strategy='adaptive_clustering', random_seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            strategy: 'adaptive_clustering', 'similarity_based', 'balanced_difficulty', 'random'\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "    def extract_class_features(self, base_dataset, model, device, batch_size=128):\n",
    "        \"\"\"\n",
    "        Extract representative features for each class using a pretrained model\n",
    "        or initial model state. This helps create semantically meaningful task splits.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Get labels\n",
    "        if hasattr(base_dataset, 'targets'):\n",
    "            labels = base_dataset.targets\n",
    "        elif hasattr(base_dataset, 'labels'):\n",
    "            labels = base_dataset.labels\n",
    "        else:\n",
    "            labels = [base_dataset[i][1] for i in range(len(base_dataset))]\n",
    "        \n",
    "        classes = sorted(set(labels))\n",
    "        class_features = {}\n",
    "        \n",
    "        # Group samples by class\n",
    "        class_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            class_to_indices[label].append(idx)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for cls in classes:\n",
    "                indices = class_to_indices[cls][:min(100, len(class_to_indices[cls]))]  # Sample max 100 per class\n",
    "                \n",
    "                features_list = []\n",
    "                for i in range(0, len(indices), batch_size):\n",
    "                    batch_indices = indices[i:i+batch_size]\n",
    "                    batch_data = torch.stack([base_dataset[idx][0] for idx in batch_indices])\n",
    "                    batch_data = batch_data.to(device)\n",
    "                    \n",
    "                    _, features = model(batch_data)  # Assuming model returns (logits, features)\n",
    "                    features_list.append(features.cpu())\n",
    "                \n",
    "                all_features = torch.cat(features_list, dim=0)\n",
    "                class_features[cls] = all_features.mean(dim=0).numpy()  # Average feature per class\n",
    "        \n",
    "        return class_features\n",
    "    \n",
    "    def adaptive_clustering_split(self, classes, class_features, n_tasks):\n",
    "        \"\"\"\n",
    "        Use clustering on class features to group similar classes together,\n",
    "        then split clusters across tasks to maximize inter-task diversity.\n",
    "        \"\"\"\n",
    "        if len(classes) < n_tasks:\n",
    "            raise ValueError(f\"Number of classes ({len(classes)}) must be >= n_tasks ({n_tasks})\")\n",
    "        \n",
    "        # Extract feature matrix\n",
    "        feature_matrix = np.array([class_features[cls] for cls in classes])\n",
    "        \n",
    "        # Cluster classes into more clusters than tasks\n",
    "        n_clusters = min(n_tasks * 2, len(classes))\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_seed)\n",
    "        cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "        \n",
    "        # Group classes by cluster\n",
    "        clusters = defaultdict(list)\n",
    "        for cls, cluster_id in zip(classes, cluster_labels):\n",
    "            clusters[cluster_id].append(cls)\n",
    "        \n",
    "        # Distribute clusters across tasks to balance difficulty and diversity\n",
    "        task_assignments = [[] for _ in range(n_tasks)]\n",
    "        cluster_ids = list(clusters.keys())\n",
    "        \n",
    "        # Sort clusters by size (larger clusters first)\n",
    "        cluster_ids.sort(key=lambda x: len(clusters[x]), reverse=True)\n",
    "        \n",
    "        # Assign clusters to tasks in round-robin fashion\n",
    "        for i, cluster_id in enumerate(cluster_ids):\n",
    "            task_id = i % n_tasks\n",
    "            task_assignments[task_id].extend(clusters[cluster_id])\n",
    "        \n",
    "        return task_assignments\n",
    "    \n",
    "    def similarity_based_split(self, classes, class_features, n_tasks):\n",
    "        \"\"\"\n",
    "        Split classes based on feature similarity to minimize within-task similarity\n",
    "        and maximize between-task diversity.\n",
    "        \"\"\"\n",
    "        feature_matrix = np.array([class_features[cls] for cls in classes])\n",
    "        similarity_matrix = cosine_similarity(feature_matrix)\n",
    "        \n",
    "        task_assignments = [[] for _ in range(n_tasks)]\n",
    "        remaining_classes = set(classes)\n",
    "        \n",
    "        for task_id in range(n_tasks):\n",
    "            if not remaining_classes:\n",
    "                break\n",
    "                \n",
    "            if task_id == 0:\n",
    "                # Start with a random class for first task\n",
    "                selected = random.choice(list(remaining_classes))\n",
    "                task_assignments[task_id].append(selected)\n",
    "                remaining_classes.remove(selected)\n",
    "            \n",
    "            # For each task, select classes that are diverse from already assigned classes\n",
    "            target_size = len(classes) // n_tasks + (1 if task_id < len(classes) % n_tasks else 0)\n",
    "            \n",
    "            while len(task_assignments[task_id]) < target_size and remaining_classes:\n",
    "                # Find class with minimum similarity to current task classes\n",
    "                best_class = None\n",
    "                min_max_similarity = float('inf')\n",
    "                \n",
    "                for candidate in remaining_classes:\n",
    "                    candidate_idx = classes.index(candidate)\n",
    "                    max_similarity = 0\n",
    "                    \n",
    "                    for assigned_class in task_assignments[task_id]:\n",
    "                        assigned_idx = classes.index(assigned_class)\n",
    "                        similarity = similarity_matrix[candidate_idx][assigned_idx]\n",
    "                        max_similarity = max(max_similarity, similarity)\n",
    "                    \n",
    "                    if max_similarity < min_max_similarity:\n",
    "                        min_max_similarity = max_similarity\n",
    "                        best_class = candidate\n",
    "                \n",
    "                if best_class:\n",
    "                    task_assignments[task_id].append(best_class)\n",
    "                    remaining_classes.remove(best_class)\n",
    "        \n",
    "        # Assign any remaining classes\n",
    "        for i, remaining_class in enumerate(remaining_classes):\n",
    "            task_assignments[i % n_tasks].append(remaining_class)\n",
    "            \n",
    "        return task_assignments\n",
    "    \n",
    "    def balanced_difficulty_split(self, base_dataset, classes, n_tasks):\n",
    "        \"\"\"\n",
    "        Split classes to balance task difficulty based on class frequency\n",
    "        and intra-class variance.\n",
    "        \"\"\"\n",
    "        # Get labels\n",
    "        if hasattr(base_dataset, 'targets'):\n",
    "            labels = base_dataset.targets\n",
    "        elif hasattr(base_dataset, 'labels'):\n",
    "            labels = base_dataset.labels\n",
    "        else:\n",
    "            labels = [base_dataset[i][1] for i in range(len(base_dataset))]\n",
    "        \n",
    "        # Calculate class statistics\n",
    "        class_stats = {}\n",
    "        for cls in classes:\n",
    "            class_indices = [i for i, label in enumerate(labels) if label == cls]\n",
    "            class_stats[cls] = {\n",
    "                'count': len(class_indices),\n",
    "                'difficulty': 1.0 / len(class_indices)  # Inverse frequency as difficulty proxy\n",
    "            }\n",
    "        \n",
    "        # Sort classes by difficulty\n",
    "        sorted_classes = sorted(classes, key=lambda x: class_stats[x]['difficulty'], reverse=True)\n",
    "        \n",
    "        # Distribute classes to balance total difficulty per task\n",
    "        task_assignments = [[] for _ in range(n_tasks)]\n",
    "        task_difficulties = [0.0 for _ in range(n_tasks)]\n",
    "        \n",
    "        for cls in sorted_classes:\n",
    "            # Assign to task with minimum current difficulty\n",
    "            min_task = min(range(n_tasks), key=lambda x: task_difficulties[x])\n",
    "            task_assignments[min_task].append(cls)\n",
    "            task_difficulties[min_task] += class_stats[cls]['difficulty']\n",
    "        \n",
    "        return task_assignments\n",
    "    \n",
    "    def create_task_loaders(self, base_dataset, n_tasks, batch_size=64, \n",
    "                           model=None, device=None, shuffle_tasks=True):\n",
    "        \"\"\"\n",
    "        Create improved task loaders using the specified splitting strategy.\n",
    "        \"\"\"\n",
    "        # Get labels and classes\n",
    "        if hasattr(base_dataset, 'targets'):\n",
    "            labels = base_dataset.targets\n",
    "        elif hasattr(base_dataset, 'labels'):\n",
    "            labels = base_dataset.labels\n",
    "        else:\n",
    "            labels = [base_dataset[i][1] for i in range(len(base_dataset))]\n",
    "        \n",
    "        classes = sorted(set(labels))\n",
    "        num_classes = len(classes)\n",
    "        \n",
    "        if n_tasks > num_classes:\n",
    "            raise ValueError(f\"n_tasks ({n_tasks}) > number of classes ({num_classes})\")\n",
    "        \n",
    "        # Apply splitting strategy\n",
    "        if self.strategy == 'adaptive_clustering' and model is not None:\n",
    "            class_features = self.extract_class_features(base_dataset, model, device)\n",
    "            task_class_assignments = self.adaptive_clustering_split(classes, class_features, n_tasks)\n",
    "        elif self.strategy == 'similarity_based' and model is not None:\n",
    "            class_features = self.extract_class_features(base_dataset, model, device)\n",
    "            task_class_assignments = self.similarity_based_split(classes, class_features, n_tasks)\n",
    "        elif self.strategy == 'balanced_difficulty':\n",
    "            task_class_assignments = self.balanced_difficulty_split(base_dataset, classes, n_tasks)\n",
    "        else:  # fallback to random\n",
    "            if shuffle_tasks:\n",
    "                random.shuffle(classes)\n",
    "            classes_per_task = num_classes // n_tasks\n",
    "            remainder = num_classes % n_tasks\n",
    "            task_class_assignments = []\n",
    "            idx = 0\n",
    "            for t in range(n_tasks):\n",
    "                count = classes_per_task + (1 if t < remainder else 0)\n",
    "                task_class_assignments.append(classes[idx:idx + count])\n",
    "                idx += count\n",
    "        \n",
    "        # Create class to indices mapping\n",
    "        class_to_indices = {cls: [] for cls in classes}\n",
    "        for idx, label in enumerate(labels):\n",
    "            class_to_indices[label].append(idx)\n",
    "        \n",
    "        # Create task splits\n",
    "        task_splits = []\n",
    "        for task_classes in task_class_assignments:\n",
    "            indices = []\n",
    "            for cls in task_classes:\n",
    "                indices.extend(class_to_indices[cls])\n",
    "            if shuffle_tasks:\n",
    "                random.shuffle(indices)\n",
    "            task_splits.append(indices)\n",
    "        \n",
    "        # Custom Dataset class\n",
    "        class TaskDataset(Dataset):\n",
    "            def __init__(self, base_dataset, indices):\n",
    "                self.base_dataset = base_dataset\n",
    "                self.indices = indices\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.indices)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                return self.base_dataset[self.indices[idx]]\n",
    "        \n",
    "        # Create loader functions\n",
    "        def train_loader_fn(task_id):\n",
    "            dataset = TaskDataset(base_dataset, task_splits[task_id])\n",
    "            return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        def test_loader_fn(task_id):\n",
    "            dataset = TaskDataset(base_dataset, task_splits[task_id])\n",
    "            return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader_fn, test_loader_fn, task_class_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc68e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class OnlineStreamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a dataset to simulate an online data stream for continual learning.\n",
    "    Each task is a slice of the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, task_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_dataset: The full dataset (e.g., torchvision.datasets.CIFAR10).\n",
    "            task_indices: List of indices for the current task's data.\n",
    "        \"\"\"\n",
    "        self.base_dataset = base_dataset\n",
    "        self.task_indices = task_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.task_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.task_indices[idx]\n",
    "        return self.base_dataset[real_idx]\n",
    "\n",
    "def create_online_stream_loaders(base_dataset, n_tasks, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into n_tasks and returns train and test loader functions.\n",
    "    \"\"\"\n",
    "    total_len = len(base_dataset)\n",
    "    indices = torch.randperm(total_len).tolist()\n",
    "    task_splits = [indices[i::n_tasks] for i in range(n_tasks)]\n",
    "\n",
    "    def train_loader_fn(task_id):\n",
    "        dataset = OnlineStreamDataset(base_dataset, task_splits[task_id])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    def test_loader_fn(task_id):\n",
    "        dataset = OnlineStreamDataset(base_dataset, task_splits[task_id])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader_fn, test_loader_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0ea8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class OnlineStreamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a dataset to simulate an online data stream for continual learning.\n",
    "    Each task is a slice of the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, task_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_dataset: The full dataset (e.g., torchvision.datasets.CIFAR10).\n",
    "            task_indices: List of indices for the current task's data.\n",
    "        \"\"\"\n",
    "        self.base_dataset = base_dataset\n",
    "        self.task_indices = task_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.task_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.task_indices[idx]\n",
    "        return self.base_dataset[real_idx]\n",
    "\n",
    "def create_online_stream_loaders(base_dataset, n_tasks, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into n_tasks and returns train and test loader functions.\n",
    "    \"\"\"\n",
    "    total_len = len(base_dataset)\n",
    "    indices = torch.randperm(total_len).tolist()\n",
    "    task_splits = [indices[i::n_tasks] for i in range(n_tasks)]\n",
    "\n",
    "    def train_loader_fn(task_id):\n",
    "        dataset = OnlineStreamDataset(base_dataset, task_splits[task_id])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    def test_loader_fn(task_id):\n",
    "        dataset = OnlineStreamDataset(base_dataset, task_splits[task_id])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader_fn, test_loader_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b59f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def create_offline_task_loaders(train_dataset, test_dataset, n_tasks=5, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits the datasets into n_tasks for offline continual learning.\n",
    "    Returns two functions: train_loader_fn(task_id), test_loader_fn(task_id).\n",
    "    \"\"\"\n",
    "    train_len = len(train_dataset)\n",
    "    test_len = len(test_dataset)\n",
    "    \n",
    "    train_indices = torch.randperm(train_len).tolist()\n",
    "    test_indices = torch.randperm(test_len).tolist()\n",
    "    \n",
    "    train_task_splits = [train_indices[i::n_tasks] for i in range(n_tasks)]\n",
    "    test_task_splits = [test_indices[i::n_tasks] for i in range(n_tasks)]\n",
    "\n",
    "    class TaskDataset(Dataset):\n",
    "        def __init__(self, base_dataset, indices):\n",
    "            self.base_dataset = base_dataset\n",
    "            self.indices = indices\n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.base_dataset[self.indices[idx]]\n",
    "\n",
    "    def train_loader_fn(task_id):\n",
    "        dataset = TaskDataset(train_dataset, train_task_splits[task_id])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    def test_loader_fn(task_id):\n",
    "        dataset = TaskDataset(test_dataset, test_task_splits[task_id])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader_fn, test_loader_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245c7fe",
   "metadata": {},
   "source": [
    "# Out of Distribution Detection\n",
    "For OOD task, we will consider 3 different approaches:\n",
    "- First, we will use Energy Based OOD Model (Deep Learning Approach)\n",
    "- Second, we will use Mahalanobis Distance OOD\n",
    "- Finally, we will use confidence based approach: Softmax-Based OOD\n",
    "## Energy Based OOD Detection\n",
    "Write a lot of function and calculation here\n",
    "The formular is E_c = - logsumexp(f_c(x)) to calculate energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f474c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EnergyDetector:\n",
    "    def __init__(self):\n",
    "        self.tau = None\n",
    "\n",
    "    def fit(self, model, buffer, device, reg_cov=1e-5):\n",
    "        # No parameters to fit for energy-based detector, as it relies directly on the model's logits.\n",
    "        # This method is included for structural compatibility with MahalanobisDetector.\n",
    "        pass\n",
    "\n",
    "    def score(self, model, x, device):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(x) # Model returns (logits, features)\n",
    "        # Compute energy score: -logsumexp(logits, dim=1)\n",
    "        energies = -torch.logsumexp(logits, dim=1)\n",
    "        return energies\n",
    "\n",
    "    def detect(self, model, x, device):\n",
    "        return self.score(model, x, device) > self.tau\n",
    "\n",
    "    def set_threshold(self, model, buffer, device, false_positive_rate=0.2):\n",
    "        X_id, _ = buffer.get_all_data()\n",
    "        if X_id is None:\n",
    "            return\n",
    "        e_id = []\n",
    "        for i in range(0, len(X_id), 256):\n",
    "            batch = X_id[i:i+256].to(device).float()\n",
    "            e_id.append(self.score(model, batch, device))\n",
    "        e_id = torch.cat(e_id)\n",
    "        self.tau = torch.quantile(e_id, 1 - false_positive_rate).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599b552",
   "metadata": {},
   "source": [
    "## Mahalanobis Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0d4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MahalanobisDetector:\n",
    "    def __init__(self):\n",
    "        self.class_means = {}\n",
    "        self.precision = None\n",
    "        self.tau = None\n",
    "\n",
    "    def fit(self, model, buffer, device, reg_cov=1e-5):\n",
    "        model.eval()\n",
    "        X, Y = buffer.get_all_data()\n",
    "        if X is None:\n",
    "            return\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        feats = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X), 256):\n",
    "                batch = X[i:i+256].float()\n",
    "                f, _ = model(batch) # Model returns (logits, features)\n",
    "                feats.append(f)\n",
    "        feats = torch.cat(feats, dim=0)\n",
    "        for c in torch.unique(Y):\n",
    "            self.class_means[int(c.item())] = feats[Y==c].mean(dim=0)\n",
    "        centered = feats - torch.stack([self.class_means[int(y.item())] for y in Y])\n",
    "        cov = (centered.t() @ centered) / (len(Y)-1)\n",
    "        cov += reg_cov * torch.eye(cov.size(0)).to(device)\n",
    "        self.precision = torch.inverse(cov)\n",
    "\n",
    "    def score(self, model, x, device):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, f = model(x) # Model returns (logits, features)\n",
    "        dists = []\n",
    "        for mu in self.class_means.values():\n",
    "            diff = f - mu.unsqueeze(0)\n",
    "            dists.append(torch.sum(diff @ self.precision * diff, dim=1))\n",
    "        dists = torch.stack(dists, dim=1)\n",
    "        # print(f\"dists={dists}\")\n",
    "        return dists.min(dim=1)[0]\n",
    "\n",
    "    def detect(self, model, x, device):\n",
    "        return self.score(model, x, device) > self.tau\n",
    "\n",
    "    def set_threshold(self, model, buffer, device, false_positive_rate=0.2):\n",
    "        X_id, _ = buffer.get_all_data()\n",
    "        if X_id is None:\n",
    "            return\n",
    "        d_id = []\n",
    "        for i in range(0, len(X_id), 256):\n",
    "            d_id.append(self.score(model, X_id[i:i+256].to(device), device))\n",
    "        d_id = torch.cat(d_id)\n",
    "        self.tau = torch.quantile(d_id, 1 - false_positive_rate).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8ce7b",
   "metadata": {},
   "source": [
    "## Softmax Based OOD Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513b3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SoftmaxDetector:\n",
    "    def __init__(self):\n",
    "        self.tau = None\n",
    "\n",
    "    def fit(self, model, buffer, device, reg_cov=1e-5):\n",
    "        # No parameters to fit for softmax-based detector, as it relies directly on the model's logits.\n",
    "        # This method is included for structural compatibility with other detectors.\n",
    "        pass\n",
    "\n",
    "    def score(self, model, x, device):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(x) # Model returns (logits, features)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        max_probs = probs.max(dim=1)[0]\n",
    "        # Score is -max_prob: lower (more negative) for ID (high confidence), higher (less negative) for OOD (low confidence)\n",
    "        return -max_probs\n",
    "\n",
    "    def detect(self, model, x, device):\n",
    "        return self.score(model, x, device) > self.tau\n",
    "\n",
    "    def set_threshold(self, model, buffer, device, false_positive_rate=0.2):\n",
    "        X_id, _ = buffer.get_all_data()\n",
    "        if X_id is None:\n",
    "            return\n",
    "        s_id = []\n",
    "        for i in range(0, len(X_id), 256):\n",
    "            batch = X_id[i:i+256].to(device).float()\n",
    "            s_id.append(self.score(model, batch, device))\n",
    "        s_id = torch.cat(s_id)\n",
    "        self.tau = torch.quantile(s_id, 1 - false_positive_rate).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a46584",
   "metadata": {},
   "source": [
    "# Task Boundary Detector\n",
    "For this one, we may disable in Offline CL\n",
    "## Loss Based (Online-LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea13648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class LossBasedTaskBoundaryDetector:\n",
    "    \"\"\"A module for detecting task boundaries in online CL using loss-based signals.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, loss_fn=nn.CrossEntropyLoss(), window_size=100, \n",
    "                 threshold_factor=2.0, buffer_size=1000):\n",
    "        \"\"\"\n",
    "        Initialize the task boundary detector.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model (nn.Module) to monitor for loss changes.\n",
    "            loss_fn: Loss function (default: CrossEntropyLoss).\n",
    "            window_size: Number of samples for sliding window loss averaging.\n",
    "            threshold_factor: Multiplier for dynamic threshold (mean + factor * std).\n",
    "            buffer_size: Maximum size of replay buffer for storing past task samples.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.window_size = window_size\n",
    "        self.threshold_factor = threshold_factor\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        # Track losses in a sliding window\n",
    "        self.loss_window = deque(maxlen=window_size)\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.task_id = 0  # Current task ID\n",
    "        self.mean_loss = 0.0\n",
    "        self.std_loss = 1.0  # Avoid division by zero\n",
    "        \n",
    "    def update_loss(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Compute and store loss for a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch).\n",
    "            targets: Target tensor (batch).\n",
    "        \n",
    "        Returns:\n",
    "            float: Current batch loss.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.model(inputs) # Model returns (logits, features)\n",
    "            loss = self.loss_fn(logits, targets).item()\n",
    "        self.loss_window.append(loss)\n",
    "        \n",
    "        # Update running mean and std for dynamic threshold\n",
    "        if len(self.loss_window) >= self.window_size // 2:\n",
    "            self.mean_loss = np.mean(list(self.loss_window))\n",
    "            self.std_loss = np.std(list(self.loss_window)) or 1.0  # Avoid zero std\n",
    "        return loss\n",
    "    \n",
    "    def detect_boundary(self, loss):\n",
    "        \"\"\"\n",
    "        Check if the current loss indicates a task boundary.\n",
    "        \n",
    "        Args:\n",
    "            loss: Current batch loss.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if a new task is detected, False otherwise.\n",
    "        \"\"\"\n",
    "        if len(self.loss_window) < self.window_size // 2:\n",
    "            return False  # Not enough data to detect reliably\n",
    "        \n",
    "        threshold = self.mean_loss + self.threshold_factor * self.std_loss\n",
    "        if loss > threshold:\n",
    "            self.task_id += 1\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def add_to_buffer(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Add samples to the replay buffer for the current task.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch).\n",
    "            targets: Target tensor (batch).\n",
    "        \"\"\"\n",
    "        for x, y in zip(inputs, targets):\n",
    "            if len(self.replay_buffer) < self.buffer_size:\n",
    "                self.replay_buffer.append((x.cpu(), y.cpu(), self.task_id))\n",
    "    \n",
    "    def get_replay_samples(self, batch_size):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of samples from the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of samples to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (inputs, targets, task_ids) or None if buffer is empty.\n",
    "        \"\"\"\n",
    "        if not self.replay_buffer:\n",
    "            return None\n",
    "        indices = np.random.choice(len(self.replay_buffer), \n",
    "                                 min(batch_size, len(self.replay_buffer)), \n",
    "                                 replace=False)\n",
    "        samples = [self.replay_buffer[i] for i in indices]\n",
    "        inputs, targets, task_ids = zip(*samples)\n",
    "        return (torch.stack(inputs), torch.stack(targets), torch.tensor(task_ids))\n",
    "    \n",
    "    def process_batch(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Process a batch of data: update loss, detect boundaries, and manage buffer.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch).\n",
    "            targets: Target tensor (batch).\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\n",
    "                'loss': Current batch loss,\n",
    "                'new_task': Whether a new task was detected,\n",
    "                'task_id': Current task ID\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Compute loss and update window\n",
    "        loss = self.update_loss(inputs, targets)\n",
    "        \n",
    "        # Detect task boundary\n",
    "        new_task = self.detect_boundary(loss)\n",
    "        \n",
    "        # Add to replay buffer\n",
    "        self.add_to_buffer(inputs, targets)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'new_task': new_task,\n",
    "            'task_id': self.task_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83549b0",
   "metadata": {},
   "source": [
    "## AutoEncoder Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abecc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Simple autoencoder for task boundary detection.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Assuming normalized inputs [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "class AutoencoderBasedTaskBoundaryDetector:\n",
    "    \"\"\"A module for detecting task boundaries in online CL using autoencoder reconstruction errors.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=128, window_size=100, \n",
    "                 threshold_factor=2.0, buffer_size=1000, lr=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the autoencoder-based task boundary detector.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Flattened input dimension (e.g., 784 for MNIST 28x28).\n",
    "            hidden_dim: Hidden layer size for the autoencoder.\n",
    "            window_size: Number of samples for sliding window error averaging.\n",
    "            threshold_factor: Multiplier for dynamic threshold (mean + factor * std).\n",
    "            buffer_size: Maximum size of replay buffer for storing past task samples.\n",
    "            lr: Learning rate for autoencoder training.\n",
    "        \"\"\"\n",
    "        self.autoencoder = Autoencoder(input_dim, hidden_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.autoencoder.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.window_size = window_size\n",
    "        self.threshold_factor = threshold_factor\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        # Track reconstruction errors in a sliding window\n",
    "        self.error_window = deque(maxlen=window_size)\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.task_id = 0  # Current task ID\n",
    "        self.mean_error = 0.0\n",
    "        self.std_error = 1.0  # Avoid division by zero\n",
    "        \n",
    "    def train_autoencoder(self, inputs, epochs=1):\n",
    "        \"\"\"\n",
    "        Train the autoencoder on a batch of inputs.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch).\n",
    "            epochs: Number of training epochs per batch.\n",
    "        \"\"\"\n",
    "        self.autoencoder.train()\n",
    "        for _ in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.autoencoder(inputs)\n",
    "            loss = self.loss_fn(outputs, inputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def compute_reconstruction_error(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute reconstruction error for a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch).\n",
    "        \n",
    "        Returns:\n",
    "            float: Average reconstruction error for the batch.\n",
    "        \"\"\"\n",
    "        self.autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.autoencoder(inputs)\n",
    "            error = self.loss_fn(outputs, inputs).item()\n",
    "        self.error_window.append(error)\n",
    "        \n",
    "        # Update running mean and std for dynamic threshold\n",
    "        if len(self.error_window) >= self.window_size // 2:\n",
    "            self.mean_error = np.mean(list(self.error_window))\n",
    "            self.std_error = np.std(list(self.error_window)) or 1.0  # Avoid zero std\n",
    "        return error\n",
    "    \n",
    "    def detect_boundary(self, error):\n",
    "        \"\"\"\n",
    "        Check if the current reconstruction error indicates a task boundary.\n",
    "        \n",
    "        Args:\n",
    "            error: Current batch reconstruction error.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if a new task is detected, False otherwise.\n",
    "        \"\"\"\n",
    "        if len(self.error_window) < self.window_size // 2:\n",
    "            return False  # Not enough data to detect reliably\n",
    "        \n",
    "        threshold = self.mean_error + self.threshold_factor * self.std_error\n",
    "        if error > threshold:\n",
    "            self.task_id += 1\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def add_to_buffer(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Add samples to the replay buffer for the current task.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch).\n",
    "            targets: Target tensor (batch).\n",
    "        \"\"\"\n",
    "        for x, y in zip(inputs, targets):\n",
    "            if len(self.replay_buffer) < self.buffer_size:\n",
    "                self.replay_buffer.append((x.cpu(), y.cpu(), self.task_id))\n",
    "    \n",
    "    def get_replay_samples(self, batch_size):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of samples from the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of samples to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (inputs, targets, task_ids) or None if buffer is empty.\n",
    "        \"\"\"\n",
    "        if not self.replay_buffer:\n",
    "            return None\n",
    "        indices = np.random.choice(len(self.replay_buffer), \n",
    "                                 min(batch_size, len(self.replay_buffer)), \n",
    "                                 replace=False)\n",
    "        samples = [self.replay_buffer[i] for i in indices]\n",
    "        inputs, targets, task_ids = zip(*samples)\n",
    "        return (torch.stack(inputs), torch.stack(targets), torch.tensor(task_ids))\n",
    "    \n",
    "    def process_batch(self, inputs, targets, train_ae=True):\n",
    "        \"\"\"\n",
    "        Process a batch of data: train autoencoder, compute error, detect boundaries, and manage buffer.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch, flattened).\n",
    "            targets: Target tensor (batch).\n",
    "            train_ae: Whether to train the autoencoder on this batch.\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\n",
    "                'error': Reconstruction error,\n",
    "                'new_task': Whether a new task was detected,\n",
    "                'task_id': Current task ID\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Train autoencoder if enabled\n",
    "        if train_ae:\n",
    "            self.train_autoencoder(inputs)\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        error = self.compute_reconstruction_error(inputs)\n",
    "        \n",
    "        # Detect task boundary\n",
    "        new_task = self.detect_boundary(error)\n",
    "        \n",
    "        # Add to replay buffer\n",
    "        self.add_to_buffer(inputs, targets)\n",
    "        \n",
    "        return {\n",
    "            'error': error,\n",
    "            'new_task': new_task,\n",
    "            'task_id': self.task_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f18b09",
   "metadata": {},
   "source": [
    "## Scheduled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f652303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class ScheduledTaskBoundaryDetector:\n",
    "    \"\"\"A module for detecting task boundaries in online CL using a fixed schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, switch_interval=1000, buffer_size=1000):\n",
    "        \"\"\"\n",
    "        Initialize the scheduled task boundary detector.\n",
    "        \n",
    "        Args:\n",
    "            switch_interval: Number of samples after which to trigger a task boundary.\n",
    "            buffer_size: Maximum size of replay buffer for storing past task samples.\n",
    "        \"\"\"\n",
    "        self.switch_interval = switch_interval\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        # Track sample count and task ID\n",
    "        self.sample_count = 0\n",
    "        self.task_id = 0\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "    \n",
    "    def update_count(self, batch_size):\n",
    "        \"\"\"\n",
    "        Update the sample count based on the current batch size.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of samples in the current batch.\n",
    "        \"\"\"\n",
    "        self.sample_count += batch_size\n",
    "    \n",
    "    def detect_boundary(self):\n",
    "        \"\"\"\n",
    "        Check if the current sample count indicates a task boundary based on the schedule.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if a new task is detected, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.sample_count >= self.switch_interval:\n",
    "            self.sample_count = 0  # Reset counter\n",
    "            self.task_id += 1\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def add_to_buffer(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Add samples to the replay buffer for the current task.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch).\n",
    "            targets: Target tensor (batch).\n",
    "        \"\"\"\n",
    "        for x, y in zip(inputs, targets):\n",
    "            if len(self.replay_buffer) < self.buffer_size:\n",
    "                self.replay_buffer.append((x.cpu(), y.cpu(), self.task_id))\n",
    "    \n",
    "    def get_replay_samples(self, batch_size):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of samples from the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of samples to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (inputs, targets, task_ids) or None if buffer is empty.\n",
    "        \"\"\"\n",
    "        if not self.replay_buffer:\n",
    "            return None\n",
    "        indices = np.random.choice(len(self.replay_buffer), \n",
    "                                 min(batch_size, len(self.replay_buffer)), \n",
    "                                 replace=False)\n",
    "        samples = [self.replay_buffer[i] for i in indices]\n",
    "        inputs, targets, task_ids = zip(*samples)\n",
    "        return (torch.stack(inputs), torch.stack(targets), torch.tensor(task_ids))\n",
    "    \n",
    "    def process_batch(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Process a batch of data: update sample count, detect boundaries, and manage buffer.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch).\n",
    "            targets: Target tensor (batch).\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\n",
    "                'error': None (for interface compatibility),\n",
    "                'new_task': Whether a new task was detected,\n",
    "                'task_id': Current task ID\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Update sample count\n",
    "        batch_size = inputs.size(0)\n",
    "        self.update_count(batch_size)\n",
    "        \n",
    "        # Detect task boundary\n",
    "        new_task = self.detect_boundary()\n",
    "        \n",
    "        # Add to replay buffer\n",
    "        self.add_to_buffer(inputs, targets)\n",
    "        \n",
    "        return {\n",
    "            'error': None,  # No error metric in scheduled approach\n",
    "            'new_task': new_task,\n",
    "            'task_id': self.task_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e454ef5",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "This should be fit for Online Learning\n",
    "For Offline Learning, we can use some other choice provides more precision\n",
    "## Average Gradient Episodic Memory (A-GEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a9b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class AGEMReplayBuffer:\n",
    "    def __init__(self, max_mem_size=None, device='cpu'):\n",
    "        self.max_mem_size = max_mem_size\n",
    "        self.exemplars = {}  # dict: task_id (int) -> (torch.Tensor X of shape (num_exemplars, C, H, W), torch.Tensor y of shape (num_exemplars,))\n",
    "        self.seen_tasks = set()\n",
    "        self.device = device\n",
    "\n",
    "    def construct_exemplar_set(self, model, X_new, y_new, task_id):\n",
    "        if task_id in self.seen_tasks:\n",
    "            return  # Skip if task already seen (assuming no duplicates)\n",
    "        self.seen_tasks.add(task_id)\n",
    "\n",
    "        if self.max_mem_size is not None:\n",
    "            m = self.max_mem_size // len(self.seen_tasks)\n",
    "            self.reduce_exemplar_set(m)\n",
    "            num_samples = min(m, len(X_new))\n",
    "            if num_samples > 0:\n",
    "                idx = torch.randperm(len(X_new))[:num_samples]\n",
    "                self.exemplars[task_id] = (X_new[idx].cpu(), y_new[idx].cpu())\n",
    "        else:\n",
    "            # Unlimited memory: store all\n",
    "            self.exemplars[task_id] = (X_new.cpu(), y_new.cpu())\n",
    "\n",
    "    def reduce_exemplar_set(self, m):\n",
    "        for t in list(self.exemplars.keys()):\n",
    "            if len(self.exemplars[t][0]) > m:\n",
    "                num_keep = min(m, len(self.exemplars[t][0]))\n",
    "                if num_keep > 0:\n",
    "                    idx = torch.randperm(len(self.exemplars[t][0]))[:num_keep]\n",
    "                    self.exemplars[t] = (self.exemplars[t][0][idx], self.exemplars[t][1][idx])\n",
    "                else:\n",
    "                    del self.exemplars[t]\n",
    "\n",
    "    def get_exemplar_set(self):\n",
    "        return self.get_all_data()\n",
    "\n",
    "    def get_exemplar_set_per_task(self, task_id):\n",
    "        if task_id in self.exemplars:\n",
    "            return self.exemplars[task_id][0], self.exemplars[task_id][1]\n",
    "        return None, None\n",
    "\n",
    "    def get_all_data(self):\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "        for t in self.exemplars:\n",
    "            X_list.append(self.exemplars[t][0])\n",
    "            Y_list.append(self.exemplars[t][1])\n",
    "        if not X_list:\n",
    "            return None, None\n",
    "        return torch.cat(X_list), torch.cat(Y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a56098c",
   "metadata": {},
   "source": [
    "## Incremental Classifier and Representation Learning (iCaRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "395763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class iCaRLReplayBuffer:\n",
    "    def __init__(self, max_mem_size=None, device='cpu'):\n",
    "        self.max_mem_size = max_mem_size\n",
    "        self.exemplars = {}  # dict: class_id (int) -> torch.Tensor of shape (num_exemplars, C, H, W)\n",
    "        self.class_to_task = {}  # dict: class_id -> task_id\n",
    "        self.seen_classes = set()\n",
    "        self.device = device\n",
    "\n",
    "    def _extract_features(self, model, X):\n",
    "        model.eval()\n",
    "        feats = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X), 256):\n",
    "                batch = X[i:i+256].to(self.device).float()\n",
    "                out = model.immediate_forward(batch)\n",
    "                f = F.avg_pool2d(out, 8).view(batch.size(0), -1)\n",
    "                feats.append(f.cpu())\n",
    "        return torch.cat(feats)\n",
    "\n",
    "    def _herding(self, features, class_mean, m):\n",
    "        selected_idx = []\n",
    "        remaining_idx = list(range(features.size(0)))\n",
    "        current_mean = torch.zeros_like(class_mean)\n",
    "        for k in range(m):\n",
    "            best_dist = float('inf')\n",
    "            best_i = None\n",
    "            for idx in remaining_idx:\n",
    "                hypo_mean = (current_mean * k + features[idx]) / (k + 1)\n",
    "                dist = torch.norm(hypo_mean - class_mean).item()\n",
    "                if dist < best_dist:\n",
    "                    best_dist = dist\n",
    "                    best_i = idx\n",
    "            if best_i is not None:\n",
    "                selected_idx.append(best_i)\n",
    "                current_mean = (current_mean * k + features[best_i]) / (k + 1)\n",
    "                remaining_idx.remove(best_i)\n",
    "        return selected_idx\n",
    "\n",
    "    def construct_exemplar_set(self, model, X_new, y_new, task_id):\n",
    "        new_classes = torch.unique(y_new).tolist()\n",
    "        for c in new_classes:\n",
    "            if c in self.seen_classes:\n",
    "                continue  # Skip if class already seen (assuming no duplicates)\n",
    "            self.seen_classes.add(c)\n",
    "            self.class_to_task[c] = task_id\n",
    "\n",
    "        if self.max_mem_size is not None:\n",
    "            m = self.max_mem_size // len(self.seen_classes)\n",
    "            self.reduce_exemplar_set(m)\n",
    "            for c in new_classes:\n",
    "                mask = (y_new == c)\n",
    "                X_c = X_new[mask]\n",
    "                if len(X_c) <= m:\n",
    "                    self.exemplars[c] = X_c\n",
    "                else:\n",
    "                    features = self._extract_features(model, X_c)\n",
    "                    class_mean = features.mean(dim=0)\n",
    "                    selected_idx = self._herding(features, class_mean, m)\n",
    "                    self.exemplars[c] = X_c[selected_idx]\n",
    "        else:\n",
    "            # Unlimited memory: store all\n",
    "            for c in new_classes:\n",
    "                mask = (y_new == c)\n",
    "                X_c = X_new[mask]\n",
    "                self.exemplars[c] = X_c\n",
    "\n",
    "    def reduce_exemplar_set(self, m):\n",
    "        for c in list(self.exemplars.keys()):\n",
    "            if len(self.exemplars[c]) > m:\n",
    "                self.exemplars[c] = self.exemplars[c][:m]\n",
    "\n",
    "    def get_exemplar_set(self):\n",
    "        return self.get_all_data()\n",
    "\n",
    "    def get_exemplar_set_per_task(self, task_id):\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "        for c, ex in self.exemplars.items():\n",
    "            if self.class_to_task.get(c) == task_id:\n",
    "                X_list.append(ex)\n",
    "                Y_list.append(torch.full((len(ex),), c, dtype=torch.long))\n",
    "        if not X_list:\n",
    "            return None, None\n",
    "        return torch.cat(X_list), torch.cat(Y_list)\n",
    "\n",
    "    def get_all_data(self):\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "        for c, ex in self.exemplars.items():\n",
    "            X_list.append(ex)\n",
    "            Y_list.append(torch.full((len(ex),), c, dtype=torch.long))\n",
    "        if not X_list:\n",
    "            return None, None\n",
    "        return torch.cat(X_list), torch.cat(Y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2399284",
   "metadata": {},
   "source": [
    "## Dark Experience Replay (DER++)\n",
    "This one store logits too, consider rewrite the model structure (OR just keep it, logits is too computational expensive for A-GEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a77a90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DERppReplayBuffer:\n",
    "    def __init__(self, max_mem_size=None, device='cpu'):\n",
    "        self.max_mem_size = max_mem_size\n",
    "        self.exemplars = {}  # dict: task_id (int) -> (torch.Tensor X of shape (num_exemplars, C, H, W), torch.Tensor y of shape (num_exemplars,), torch.Tensor logits of shape (num_exemplars, n_classes))\n",
    "        self.seen_tasks = set()\n",
    "        self.device = device\n",
    "        self.n_classes = None  # To be set when storing exemplars to track output dimension\n",
    "\n",
    "    def construct_exemplar_set(self, model, X_new, y_new, task_id):\n",
    "        if task_id in self.seen_tasks:\n",
    "            return  # Skip if task already seen (assuming no duplicates)\n",
    "        self.seen_tasks.add(task_id)\n",
    "\n",
    "        # Compute logits for the new data\n",
    "        model.eval()\n",
    "        logits = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X_new), 256):\n",
    "                batch = X_new[i:i+256].to(self.device).float()\n",
    "                batch_logits, _ = model(batch) # Model returns (logits, features)\n",
    "                logits.append(batch_logits.cpu())\n",
    "        logits = torch.cat(logits)\n",
    "\n",
    "        # Initialize n_classes if not set\n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = logits.size(1)\n",
    "\n",
    "        if self.max_mem_size is not None:\n",
    "            m = self.max_mem_size // len(self.seen_tasks)\n",
    "            self.reduce_exemplar_set(m)\n",
    "            num_samples = min(m, len(X_new))\n",
    "            if num_samples > 0:\n",
    "                idx = torch.randperm(len(X_new))[:num_samples]\n",
    "                self.exemplars[task_id] = (X_new[idx].cpu(), y_new[idx].cpu(), logits[idx].cpu())\n",
    "        else:\n",
    "            # Unlimited memory: store all\n",
    "            self.exemplars[task_id] = (X_new.cpu(), y_new.cpu(), logits.cpu())\n",
    "\n",
    "    def reduce_exemplar_set(self, m):\n",
    "        for t in list(self.exemplars.keys()):\n",
    "            if len(self.exemplars[t][0]) > m:\n",
    "                num_keep = min(m, len(self.exemplars[t][0]))\n",
    "                if num_keep > 0:\n",
    "                    idx = torch.randperm(len(self.exemplars[t][0]))[:num_keep]\n",
    "                    self.exemplars[t] = (\n",
    "                        self.exemplars[t][0][idx],\n",
    "                        self.exemplars[t][1][idx],\n",
    "                        self.exemplars[t][2][idx]\n",
    "                    )\n",
    "                else:\n",
    "                    del self.exemplars[t]\n",
    "\n",
    "    def get_exemplar_set(self):\n",
    "        return self.get_all_data()\n",
    "\n",
    "    def get_exemplar_set_per_task(self, task_id):\n",
    "        if task_id in self.exemplars:\n",
    "            return self.exemplars[task_id][0], self.exemplars[task_id][1], self.exemplars[task_id][2]\n",
    "        return None, None, None\n",
    "\n",
    "    def get_all_data(self):\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "        logits_list = []\n",
    "        for t in self.exemplars:\n",
    "            X_list.append(self.exemplars[t][0])\n",
    "            Y_list.append(self.exemplars[t][1])\n",
    "            logits_list.append(self.exemplars[t][2])\n",
    "        if not X_list:\n",
    "            return None, None, None\n",
    "        return torch.cat(X_list), torch.cat(Y_list), torch.cat(logits_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ac425",
   "metadata": {},
   "source": [
    "# Regularization Mechanism\n",
    "Let's go with EWC for Offline and Online then Online EWC and SI\n",
    "## Elastic Weight Consolidation (EWC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "429b198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "\n",
    "class EWC:\n",
    "    \"\"\"Implements Elastic Weight Consolidation for a single task.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device, ewc_lambda: float = 1.0):\n",
    "        \"\"\"Initialize EWC with a model and device.\n",
    "        \n",
    "        Args:\n",
    "            model (nn.Module): The neural network model.\n",
    "            device (torch.device): Device to store parameters and Fisher matrices.\n",
    "            ewc_lambda (float): Regularization strength.\n",
    "        \"\"\"\n",
    "        self.model = model  # Don't deepcopy, use reference to current model\n",
    "        self.device = device\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.params = {n: p.clone().detach().to(device) for n, p in model.named_parameters() if p.requires_grad}\n",
    "        self.fisher = None\n",
    "    \n",
    "    def update_in_training(self, *args, **kwargs):\n",
    "        \"\"\"EWC doesn't update during training - method for compatibility.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update_per_task(self, dataloader, criterion, samples: int = 1000, *args, **kwargs):\n",
    "        \"\"\"Compute Fisher Information Matrix after task completion.\n",
    "        \n",
    "        Args:\n",
    "            dataloader: DataLoader for the task dataset.\n",
    "            criterion: Loss function (e.g., nn.CrossEntropyLoss).\n",
    "            samples (int): Number of samples to estimate Fisher information.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        fisher = {n: torch.zeros_like(p).to(self.device) for n, p in self.params.items()}\n",
    "        count = 0\n",
    "        \n",
    "        for data, target in dataloader:\n",
    "            if count >= samples:\n",
    "                break\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            logits, _ = self.model(data) # Model returns (logits, features)\n",
    "            \n",
    "            loss = criterion(logits, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.grad is not None and n in fisher:\n",
    "                    fisher[n] += p.grad.data.pow(2)\n",
    "            count += data.size(0)\n",
    "        \n",
    "        if count > 0:\n",
    "            self.fisher = {n: f / count for n, f in fisher.items()}\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Update stored parameters after computing Fisher\n",
    "        self.params = {n: p.clone().detach().to(self.device) for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "    \n",
    "    def penalty(self):\n",
    "        \"\"\"Compute EWC regularization penalty.\"\"\"\n",
    "        if self.fisher is None:\n",
    "            return torch.tensor(0.0).to(self.device)\n",
    "        \n",
    "        penalty = torch.tensor(0.0).to(self.device)\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if n in self.params and n in self.fisher:\n",
    "                penalty += (self.fisher[n] * (p - self.params[n]).pow(2)).sum()\n",
    "        return self.ewc_lambda * penalty\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save parameters and Fisher matrix to a file.\n",
    "        \n",
    "        Args:\n",
    "            path (str): File path to save the EWC state.\n",
    "        \"\"\"\n",
    "        if self.fisher is None:\n",
    "            print(f\"Warning: Fisher matrix is None, saving empty EWC state to {path}\")\n",
    "        \n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        torch.save({\n",
    "            'params': {k: v.cpu() for k, v in self.params.items()},\n",
    "            'fisher': {k: v.cpu() for k, v in self.fisher.items()} if self.fisher is not None else None\n",
    "        }, path)\n",
    "        print(f\"EWC state saved to {path}\")\n",
    "\n",
    "    # Keep backward compatibility\n",
    "    def compute_fisher(self, dataloader, criterion, samples: int = 1000):\n",
    "        \"\"\"Backward compatibility - calls update_per_task.\"\"\"\n",
    "        return self.update_per_task(dataloader, criterion, samples)\n",
    "\n",
    "class PerTaskEWC:\n",
    "    \"\"\"Manages EWC for multiple tasks by loading EWC states from file paths.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device, ewc_lambda: float = 1.0, ewc_paths: List[str] = []):\n",
    "        \"\"\"Initialize PerTaskEWC with a model, device, and list of EWC file paths.\n",
    "        \n",
    "        Args:\n",
    "            model (nn.Module): The neural network model.\n",
    "            device (torch.device): Device to store parameters and Fisher matrices.\n",
    "            ewc_lambda (float): Regularization strength.\n",
    "            ewc_paths (List[str]): List of file paths containing EWC states.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.ewc_list = []\n",
    "        self.load_ewc_paths(ewc_paths)\n",
    "    \n",
    "    def load_ewc_paths(self, ewc_paths: List[str]):\n",
    "        \"\"\"Load EWC states from a list of file paths.\n",
    "        \n",
    "        Args:\n",
    "            ewc_paths (List[str]): List of file paths to EWC state dictionaries.\n",
    "        \"\"\"\n",
    "        self.ewc_list = []\n",
    "        for path in ewc_paths:\n",
    "            if os.path.exists(path):\n",
    "                state = torch.load(path, map_location=self.device)\n",
    "                ewc = EWC(self.model, self.device, self.ewc_lambda)\n",
    "                ewc.params = {n: p.to(self.device) for n, p in state['params'].items()}\n",
    "                ewc.fisher = {n: f.to(self.device) for n, f in state['fisher'].items()} if state['fisher'] is not None else None\n",
    "                self.ewc_list.append(ewc)\n",
    "                print(f\"Loaded EWC state from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: EWC file {path} not found.\")\n",
    "    \n",
    "    def update_in_training(self, *args, **kwargs):\n",
    "        \"\"\"PerTaskEWC doesn't update during training - method for compatibility.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update_per_task(self, *args, **kwargs):\n",
    "        \"\"\"PerTaskEWC doesn't update per task - it loads from files.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def penalty(self):\n",
    "        \"\"\"Compute total EWC penalty across all tasks for the current model.\"\"\"\n",
    "        total_penalty = torch.tensor(0.0).to(self.device)\n",
    "        for ewc in self.ewc_list:\n",
    "            if ewc.fisher is not None:\n",
    "                penalty = torch.tensor(0.0).to(self.device)\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if n in ewc.params and n in ewc.fisher:\n",
    "                        penalty += (ewc.fisher[n] * (p - ewc.params[n]).pow(2)).sum()\n",
    "                total_penalty += penalty\n",
    "        return self.ewc_lambda * total_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a997c",
   "metadata": {},
   "source": [
    "## Synaptic Intelligence (SI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8804caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SynapticIntelligence(nn.Module):\n",
    "    def __init__(self, model, si_lambda=0.1):\n",
    "        super(SynapticIntelligence, self).__init__()\n",
    "        self.model = model\n",
    "        self.si_lambda = si_lambda  # Regularization strength\n",
    "        self.omega = {}  # Importance weights for parameters\n",
    "        self.prev_params = {}  # Store previous parameter values\n",
    "        self.param_importance = {}  # Accumulated importance\n",
    "        self.eps = 1e-8  # Small constant to avoid division by zero\n",
    "\n",
    "        # Initialize parameter tracking\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.omega[name] = torch.zeros_like(param)\n",
    "                self.prev_params[name] = param.data.clone().detach()\n",
    "                self.param_importance[name] = torch.zeros_like(param)\n",
    "\n",
    "    def update_in_training(self, *args, **kwargs):\n",
    "        \"\"\"Update importance during training (call after each backward pass).\n",
    "        Args are ignored for compatibility with other regularization methods.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                # Update importance: accumulate squared gradients\n",
    "                self.param_importance[name] += param.grad.data ** 2\n",
    "\n",
    "    def update_per_task(self, *args, **kwargs):\n",
    "        \"\"\"Compute importance weights (omega) after task training.\n",
    "        Args are ignored for compatibility with other regularization methods.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # Omega is the normalized importance\n",
    "                total_importance = self.param_importance[name].sum() + self.eps\n",
    "                self.omega[name] = self.param_importance[name] / total_importance\n",
    "                # Reset importance for the next task\n",
    "                self.param_importance[name].zero_()\n",
    "                # Update previous parameters\n",
    "                self.prev_params[name].copy_(param.data.clone().detach())\n",
    "\n",
    "    def penalty(self):\n",
    "        \"\"\"Compute SI regularization penalty.\"\"\"\n",
    "        penalty = 0.0\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # Penalty = sum(omega * (current_param - prev_param)^2)\n",
    "                delta = param - self.prev_params[name]\n",
    "                penalty += (self.omega[name] * (delta ** 2)).sum()\n",
    "        return self.si_lambda * penalty\n",
    "\n",
    "    # Keep backward compatibility\n",
    "    def update_importance(self, *args, **kwargs):\n",
    "        \"\"\"Backward compatibility - calls update_in_training.\"\"\"\n",
    "        return self.update_in_training(*args, **kwargs)\n",
    "    \n",
    "    def compute_omega(self):\n",
    "        \"\"\"Backward compatibility - calls update_per_task.\"\"\"\n",
    "        return self.update_per_task()\n",
    "\n",
    "    def save(self, path):\n",
    "        # This method doesn't require saving\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984026b6",
   "metadata": {},
   "source": [
    "## Online Elastic Weight Consolidation (Online EWC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3290ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class OnlineEWC(nn.Module):\n",
    "    def __init__(self, model, ewc_lambda=0.4, fisher_n=100):\n",
    "        super(OnlineEWC, self).__init__()\n",
    "        self.model = model\n",
    "        self.ewc_lambda = ewc_lambda  # Regularization strength\n",
    "        self.fisher_n = fisher_n  # Number of samples to estimate Fisher\n",
    "        self.prev_params = {}  # Store previous parameter values\n",
    "        self.fisher = {}  # Fisher Information Matrix (diagonal)\n",
    "        self.eps = 1e-8  # Small constant to avoid division by zero\n",
    "\n",
    "        # Initialize parameter tracking\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.prev_params[name] = param.data.clone().detach()\n",
    "                self.fisher[name] = torch.zeros_like(param)\n",
    "\n",
    "    def update_in_training(self, data_loader, criterion, *args, **kwargs):\n",
    "        \"\"\"Update Fisher Information Matrix during training.\n",
    "        Additional args are ignored for compatibility.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        fisher_acc = {name: torch.zeros_like(param) for name, param in self.model.named_parameters() if param.requires_grad}\n",
    "        n_samples = 0\n",
    "\n",
    "        for inputs, targets in data_loader:\n",
    "            if n_samples >= self.fisher_n:\n",
    "                break\n",
    "            inputs, targets = inputs.to(next(self.model.parameters()).device), targets.to(next(self.model.parameters()).device)\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            logits, _ = self.model(inputs) # Model returns (logits, features)\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    fisher_acc[name] += param.grad.data ** 2\n",
    "            n_samples += inputs.size(0)\n",
    "\n",
    "        # Average Fisher and update previous parameters\n",
    "        for name in fisher_acc:\n",
    "            if n_samples > 0:\n",
    "                fisher_acc[name] /= n_samples\n",
    "                self.fisher[name] = fisher_acc[name]\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def update_per_task(self, *args, **kwargs):\n",
    "        \"\"\"Update previous parameters after task completion.\n",
    "        Args are ignored for compatibility with other regularization methods.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.prev_params[name].copy_(param.data.clone().detach())\n",
    "\n",
    "    def penalty(self):\n",
    "        \"\"\"Compute EWC regularization penalty.\"\"\"\n",
    "        penalty = 0.0\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # Penalty = sum(Fisher * (current_param - prev_param)^2)\n",
    "                delta = param - self.prev_params[name]\n",
    "                penalty += (self.fisher[name] * (delta ** 2)).sum()\n",
    "        return self.ewc_lambda * penalty\n",
    "\n",
    "    # Keep backward compatibility\n",
    "    def update_fisher(self, data_loader, criterion):\n",
    "        \"\"\"Backward compatibility - calls update_in_training.\"\"\"\n",
    "        return self.update_in_training(data_loader, criterion)\n",
    "    \n",
    "    def save(self, path):\n",
    "        # This method doesn't require saving\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b40c5",
   "metadata": {},
   "source": [
    "# Foundation Model\n",
    "For this model, we will build a simple CNN to deal with the task, and energy will act as the judge. Let's go through the structure:\n",
    "## Basic Convolutional Block\n",
    "This block simply take the input and return the output. We will follow normal structure: BatchNorm2d (stablizing data) -> ReLU (activate data) -> Conv2d (filter)\n",
    "## Network Block\n",
    "Wrapper for linking blocks as neural network\n",
    "## WideResNet\n",
    "This creates a wide ResNet to capture info. The depth is computed as (at least 2 blocks per Network Block) * 3 + 4 additional blocks (conv, bn, relu, fc). We use global average pooling to reduce the image (processed from neural network) to a single value per channel, then flip to 1D value vector, finally go through classifier. In additional, we add `immediate_forward` function for getting the pattern the model learned, and a `feature_list` function that saves immediate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1db21234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Basic block\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, dropout=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.dropout = dropout\n",
    "        self.equalInOut = (in_channels == out_channels)\n",
    "        # Add a shortcut to match dimensions\n",
    "        self.convShortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False) if not self.equalInOut else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # First convolutional layer finds simple patterns\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        if self.equalInOut:\n",
    "            out = self.relu2(self.bn2(self.conv1(out)))\n",
    "        else:\n",
    "            out = self.relu2(self.bn2(self.conv1(x)))\n",
    "        \n",
    "        # Dropout to prevent overfitting\n",
    "        if self.dropout > 0:\n",
    "            out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second convolutional layer finds more complex patterns\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Residual Connection\n",
    "        if not self.equalInOut:\n",
    "            return torch.add(out, self.convShortcut(x))\n",
    "        else:\n",
    "            return torch.add(out, x)\n",
    "\n",
    "# Network block    \n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, n_blocks, stride=1, dropout=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_channels, out_channels, n_blocks, stride, dropout)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, n_blocks, stride, dropout):\n",
    "        layers = []\n",
    "        for i in range(n_blocks):\n",
    "            layers.append(block(in_channels if i == 0 else out_channels, out_channels, stride if i == 0 else 1, dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "# OOD Model\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, n_classes, widen_factor=1, dropout=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n_blocks = (depth - 4) // 6\n",
    "        block = BasicBlock\n",
    "        # Convolution before any block\n",
    "        self.conv1 = nn.Conv2d(3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(block, n_channels[0], n_channels[1], n_blocks, stride=1, dropout=dropout)\n",
    "\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(block, n_channels[1], n_channels[2], n_blocks, stride=2, dropout=dropout)\n",
    "\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(block, n_channels[2], n_channels[3], n_blocks, stride=2, dropout=dropout)\n",
    "\n",
    "        # Global average pooling & classifier\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(n_channels[3], n_classes)\n",
    "        self.n_channels = n_channels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        features = out.view(-1, self.n_channels)  # Store features\n",
    "        logits = self.fc(features)\n",
    "        return logits, features\n",
    "\n",
    "    def immediate_forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8132f9",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "Simply correction + task regularization + specific module loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd3a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Dict, Any, Union, List\n",
    "\n",
    "class ContinualLearningLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Unified loss module for continual learning that combines:\n",
    "    - Task loss (classification)\n",
    "    - Regularization penalties (EWC, SI, Online EWC)\n",
    "    - Replay loss\n",
    "    - OOD-specific losses\n",
    "    - Knowledge distillation loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 # Loss weights\n",
    "                 task_loss_weight: float = 1.0,\n",
    "                 regularization_weight: float = 1.0,\n",
    "                 replay_weight: float = 0.5,\n",
    "                 ood_weight: float = 0.1,\n",
    "                 distillation_weight: float = 0.5,\n",
    "                 \n",
    "                 # Task loss\n",
    "                 task_criterion: nn.Module = None,\n",
    "                 \n",
    "                 # Regularization mechanisms\n",
    "                 regularizers: List = None,\n",
    "                 \n",
    "                 # Replay buffer\n",
    "                 replay_buffer = None,\n",
    "                 \n",
    "                 # OOD detector\n",
    "                 ood_detector = None,\n",
    "                 \n",
    "                 # Distillation\n",
    "                 distillation_temperature: float = 4.0,\n",
    "                 \n",
    "                 # Device\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Initialize the unified loss module.\n",
    "        \n",
    "        Args:\n",
    "            task_loss_weight: Weight for main task loss\n",
    "            regularization_weight: Weight for regularization penalties\n",
    "            replay_weight: Weight for replay loss\n",
    "            ood_weight: Weight for OOD-specific losses\n",
    "            distillation_weight: Weight for knowledge distillation\n",
    "            task_criterion: Loss function for main task (default: CrossEntropyLoss)\n",
    "            regularizers: List of regularization mechanisms\n",
    "            replay_buffer: Replay buffer instance\n",
    "            ood_detector: OOD detection module\n",
    "            distillation_temperature: Temperature for knowledge distillation\n",
    "            device: Computation device\n",
    "        \"\"\"\n",
    "        super(ContinualLearningLoss, self).__init__()\n",
    "        \n",
    "        # Loss weights\n",
    "        self.task_loss_weight = task_loss_weight\n",
    "        self.regularization_weight = regularization_weight\n",
    "        self.replay_weight = replay_weight\n",
    "        self.ood_weight = ood_weight\n",
    "        self.distillation_weight = distillation_weight\n",
    "        \n",
    "        # Loss components\n",
    "        self.task_criterion = task_criterion or nn.CrossEntropyLoss()\n",
    "        self.regularizers = regularizers or []\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.ood_detector = ood_detector\n",
    "        self.distillation_temperature = distillation_temperature\n",
    "        self.device = device\n",
    "        \n",
    "        # For storing previous model (knowledge distillation)\n",
    "        self.previous_model = None\n",
    "        \n",
    "    def set_previous_model(self, model):\n",
    "        \"\"\"Set previous model for knowledge distillation.\"\"\"\n",
    "        self.previous_model = model\n",
    "        \n",
    "    def _compute_task_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute main task classification loss.\"\"\"\n",
    "        return self.task_criterion(logits, targets)\n",
    "    \n",
    "    def _compute_regularization_loss(self) -> torch.Tensor:\n",
    "        \"\"\"Compute total regularization penalty from all mechanisms.\"\"\"\n",
    "        total_penalty = torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        for regularizer in self.regularizers:\n",
    "            if hasattr(regularizer, 'penalty'):\n",
    "                penalty = regularizer.penalty()\n",
    "                total_penalty += penalty\n",
    "                \n",
    "        return total_penalty\n",
    "    \n",
    "    def _compute_replay_loss(self, model, replay_batch_size: int = 32) -> torch.Tensor:\n",
    "        \"\"\"Compute loss on replay buffer samples.\"\"\"\n",
    "        if self.replay_buffer is None:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Get replay samples\n",
    "        if hasattr(self.replay_buffer, 'get_all_data'):\n",
    "            replay_data = self.replay_buffer.get_all_data()\n",
    "            if replay_data[0] is None:  # No replay data available\n",
    "                return torch.tensor(0.0, device=self.device)\n",
    "            \n",
    "            # Handle different replay buffer types\n",
    "            if len(replay_data) == 3:  # DER++ with logits\n",
    "                X_replay, y_replay, logits_replay = replay_data\n",
    "                # Sample a batch\n",
    "                if len(X_replay) > replay_batch_size:\n",
    "                    indices = torch.randperm(len(X_replay))[:replay_batch_size]\n",
    "                    X_replay = X_replay[indices].to(self.device)\n",
    "                    y_replay = y_replay[indices].to(self.device)\n",
    "                    logits_replay = logits_replay[indices].to(self.device)\n",
    "                else:\n",
    "                    X_replay = X_replay.to(self.device)\n",
    "                    y_replay = y_replay.to(self.device)\n",
    "                    logits_replay = logits_replay.to(self.device)\n",
    "                \n",
    "                # Compute current model output\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    current_logits, _ = model(X_replay) # Model returns (logits, features)\n",
    "                \n",
    "                # DER++ loss: classification + logits distillation\n",
    "                classification_loss = self.task_criterion(current_logits, y_replay)\n",
    "                distillation_loss = F.mse_loss(current_logits, logits_replay)\n",
    "                return classification_loss + 0.5 * distillation_loss\n",
    "                \n",
    "            else:  # Standard replay (A-GEM, iCaRL)\n",
    "                X_replay, y_replay = replay_data\n",
    "                # Sample a batch\n",
    "                if len(X_replay) > replay_batch_size:\n",
    "                    indices = torch.randperm(len(X_replay))[:replay_batch_size]\n",
    "                    X_replay = X_replay[indices].to(self.device)\n",
    "                    y_replay = y_replay[indices].to(self.device)\n",
    "                else:\n",
    "                    X_replay = X_replay.to(self.device)\n",
    "                    y_replay = y_replay.to(self.device)\n",
    "                \n",
    "                # Compute loss\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    replay_logits, _ = model(X_replay) # Model returns (logits, features)\n",
    "                \n",
    "                return self.task_criterion(replay_logits, y_replay)\n",
    "        \n",
    "        return torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "    def _compute_ood_specific_loss(self, model, inputs: torch.Tensor, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute OOD detector specific losses.\"\"\"\n",
    "        if self.ood_detector is None:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Energy-based OOD loss\n",
    "        if hasattr(self.ood_detector, '__class__') and 'Energy' in self.ood_detector.__class__.__name__:\n",
    "            # Energy regularization: encourage lower energy for in-distribution samples\n",
    "            energy = -torch.logsumexp(logits, dim=1)\n",
    "            energy_loss = torch.mean(torch.relu(energy + 1.0))  # Encourage energy < -1\n",
    "            return energy_loss\n",
    "        \n",
    "        # Mahalanobis-based OOD loss\n",
    "        elif hasattr(self.ood_detector, '__class__') and 'Mahalanobis' in self.ood_detector.__class__.__name__:\n",
    "            # Feature space regularization\n",
    "            if hasattr(model, 'feature_list'):\n",
    "                _, features = model(inputs) # Model returns (logits, features)\n",
    "                if features:\n",
    "                    # Encourage compact feature representations\n",
    "                    feature_variance = torch.var(features[0].view(features[0].size(0), -1), dim=0)\n",
    "                    compactness_loss = torch.mean(feature_variance)\n",
    "                    return 0.1 * compactness_loss\n",
    "        \n",
    "        # Softmax-based OOD loss\n",
    "        elif hasattr(self.ood_detector, '__class__') and 'Softmax' in self.ood_detector.__class__.__name__:\n",
    "            # Confidence regularization: encourage high confidence for in-distribution\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            max_probs = probs.max(dim=1)[0]\n",
    "            confidence_loss = torch.mean(1.0 - max_probs)  # Encourage high confidence\n",
    "            return confidence_loss\n",
    "        \n",
    "        return torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "    def _compute_distillation_loss(self, model, inputs: torch.Tensor, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute knowledge distillation loss with previous model.\"\"\"\n",
    "        if self.previous_model is None:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        self.previous_model.eval()\n",
    "        with torch.no_grad():\n",
    "            prev_logits, _ = self.previous_model(inputs) # Model returns (logits, features)\n",
    "        \n",
    "        # Knowledge distillation loss\n",
    "        T = self.distillation_temperature\n",
    "        current_soft = F.log_softmax(logits / T, dim=1)\n",
    "        previous_soft = F.softmax(prev_logits / T, dim=1)\n",
    "        \n",
    "        kd_loss = F.kl_div(current_soft, previous_soft, reduction='batchmean') * (T ** 2)\n",
    "        return kd_loss\n",
    "    \n",
    "    def forward(self, \n",
    "                model,\n",
    "                inputs: torch.Tensor, \n",
    "                targets: torch.Tensor,\n",
    "                logits: Optional[torch.Tensor] = None,\n",
    "                replay_batch_size: int = 32,\n",
    "                return_components: bool = False) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute total loss combining all components.\n",
    "        \n",
    "        Args:\n",
    "            model: Current model\n",
    "            inputs: Input batch\n",
    "            targets: Target labels\n",
    "            logits: Pre-computed logits (optional, will compute if None)\n",
    "            replay_batch_size: Batch size for replay samples\n",
    "            return_components: Whether to return individual loss components\n",
    "            \n",
    "        Returns:\n",
    "            Total loss or dict of loss components if return_components=True\n",
    "        \"\"\"\n",
    "        # Compute logits if not provided\n",
    "        if logits is None:\n",
    "            logits, _ = model(inputs) # Model returns (logits, features)\n",
    "        \n",
    "        # Compute individual loss components\n",
    "        task_loss = self._compute_task_loss(logits, targets)\n",
    "        regularization_loss = self._compute_regularization_loss()\n",
    "        replay_loss = self._compute_replay_loss(model, replay_batch_size)\n",
    "        ood_loss = self._compute_ood_specific_loss(model, inputs, logits)\n",
    "        distillation_loss = self._compute_distillation_loss(model, inputs, logits)\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        total_loss = (self.task_loss_weight * task_loss +\n",
    "                     self.regularization_weight * regularization_loss +\n",
    "                     self.replay_weight * replay_loss +\n",
    "                     self.ood_weight * ood_loss +\n",
    "                     self.distillation_weight * distillation_loss)\n",
    "        \n",
    "        if return_components:\n",
    "            return {\n",
    "                'total_loss': total_loss,\n",
    "                'task_loss': task_loss,\n",
    "                'regularization_loss': regularization_loss,\n",
    "                'replay_loss': replay_loss,\n",
    "                'ood_loss': ood_loss,\n",
    "                'distillation_loss': distillation_loss\n",
    "            }\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def update_weights(self, **kwargs):\n",
    "        \"\"\"Update loss component weights dynamically.\"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "    \n",
    "    def add_regularizer(self, regularizer):\n",
    "        \"\"\"Add a new regularization mechanism.\"\"\"\n",
    "        self.regularizers.append(regularizer)\n",
    "    \n",
    "    def set_replay_buffer(self, replay_buffer):\n",
    "        \"\"\"Set or update the replay buffer.\"\"\"\n",
    "        self.replay_buffer = replay_buffer\n",
    "    \n",
    "    def set_ood_detector(self, ood_detector):\n",
    "        \"\"\"Set or update the OOD detector.\"\"\"\n",
    "        self.ood_detector = ood_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b861553",
   "metadata": {},
   "source": [
    "# Online Continual Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d751acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class OnlineCLTrainer:\n",
    "    \"\"\"\n",
    "    Orchestrates Online Continual Learning training and evaluation using the best modules from above.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader_fn,  # function(task_id) -> DataLoader\n",
    "        test_loader_fn,   # function(task_id) -> DataLoader\n",
    "        device,\n",
    "        n_tasks,\n",
    "        n_classes,\n",
    "        evaluating_result=True,\n",
    "        epochs_per_task=1,\n",
    "        lr=0.01,\n",
    "        buffer_size=1000,\n",
    "        max_mem_size=2000,\n",
    "        regularizer_type='OnlineEWC'\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.n_tasks = n_tasks\n",
    "        self.n_classes = n_classes\n",
    "        self.evaluating_result = evaluating_result\n",
    "        self.epochs_per_task = epochs_per_task\n",
    "        self.lr = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.max_mem_size = max_mem_size\n",
    "\n",
    "        # Replay Buffer: DER++ (best for online CL)\n",
    "        self.replay_buffer = DERppReplayBuffer(max_mem_size=max_mem_size, device=device)\n",
    "\n",
    "        # OOD Detector: Energy-based\n",
    "        self.ood_detector = EnergyDetector()\n",
    "\n",
    "        # Task Boundary Detector: Loss-based\n",
    "        self.task_boundary_detector = LossBasedTaskBoundaryDetector(\n",
    "            model=self.model, window_size=100, buffer_size=buffer_size\n",
    "        )\n",
    "\n",
    "        # Regularization: Online EWC or SI\n",
    "        if regularizer_type == 'OnlineEWC':\n",
    "            self.regularizer = OnlineEWC(self.model, ewc_lambda=0.4)\n",
    "        else:\n",
    "            self.regularizer = SynapticIntelligence(self.model, si_lambda=0.1)\n",
    "\n",
    "        # Loss Function\n",
    "        self.loss_fn = ContinualLearningLoss(\n",
    "            task_criterion=torch.nn.CrossEntropyLoss(),\n",
    "            regularizers=[self.regularizer],\n",
    "            replay_buffer=self.replay_buffer,\n",
    "            ood_detector=self.ood_detector,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Data loader functions\n",
    "        self.train_loader_fn = train_loader_fn\n",
    "        self.test_loader_fn = test_loader_fn\n",
    "\n",
    "        # Enhanced tracking\n",
    "        self.task_acc_history = []\n",
    "        self.time_acc_history = []\n",
    "        self.task_ids = []\n",
    "        self.time_steps = []\n",
    "        self.loss_history = []\n",
    "        self.task_loss_components = []  # Store detailed loss breakdowns per task\n",
    "\n",
    "    def evaluate_detailed(self, task_id):\n",
    "        \"\"\"Enhanced evaluation with detailed metrics\"\"\"\n",
    "        self.model.eval()\n",
    "        loader = self.test_loader_fn(task_id)\n",
    "        correct, total = 0, 0\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                logits, _ = self.model(x)\n",
    "                \n",
    "                # Accuracy calculation\n",
    "                pred = logits.argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "                \n",
    "                # Loss calculation\n",
    "                loss_components = self.loss_fn(self.model, x, y, logits, return_components=True)\n",
    "                total_loss += loss_components['total_loss'].item()\n",
    "                batch_count += 1\n",
    "        \n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0.0\n",
    "        \n",
    "        return acc, avg_loss\n",
    "\n",
    "    def evaluate(self, task_id):\n",
    "        \"\"\"Simple evaluation for backward compatibility\"\"\"\n",
    "        acc, _ = self.evaluate_detailed(task_id)\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        print(f\"Starting Online Continual Learning with {self.n_tasks} tasks\")\n",
    "        print(f\"Model: {self.model.__class__.__name__}\")\n",
    "        print(f\"Regularizer: {self.regularizer.__class__.__name__}\")\n",
    "        print(f\"Replay Buffer: {self.replay_buffer.__class__.__name__}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        global_step = 0\n",
    "        for task_id in range(self.n_tasks):\n",
    "            print(f\"\\n🔄 Starting Task {task_id+1}/{self.n_tasks}\")\n",
    "            train_loader = self.train_loader_fn(task_id)\n",
    "            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n",
    "            \n",
    "            task_losses = []\n",
    "            epoch_loss_components = []\n",
    "            \n",
    "            for epoch in range(self.epochs_per_task):\n",
    "                epoch_loss = 0.0\n",
    "                batch_count = 0\n",
    "                epoch_loss_breakdown = {'total': 0, 'task': 0, 'regularization': 0, 'replay': 0, 'ood': 0, 'distillation': 0}\n",
    "                \n",
    "                print(f\"  📊 Epoch {epoch+1}/{self.epochs_per_task}\")\n",
    "                \n",
    "                for batch_idx, (x, y) in enumerate(train_loader):\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "                    self.model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    logits, _ = self.model(x)\n",
    "                    loss_components = self.loss_fn(self.model, x, y, logits, return_components=True)\n",
    "                    loss = loss_components['total_loss']\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Track losses\n",
    "                    epoch_loss += loss.item()\n",
    "                    for key in epoch_loss_breakdown:\n",
    "                        if key == 'total':\n",
    "                            epoch_loss_breakdown[key] += loss.item()\n",
    "                        else:\n",
    "                            component_key = f'{key}_loss'\n",
    "                            if component_key in loss_components:\n",
    "                                epoch_loss_breakdown[key] += loss_components[component_key].item()\n",
    "                    \n",
    "                    # Update regularizer\n",
    "                    if hasattr(self.regularizer, 'update_in_training'):\n",
    "                        self.regularizer.update_in_training(train_loader, torch.nn.CrossEntropyLoss())\n",
    "                    \n",
    "                    # Update replay buffer\n",
    "                    self.replay_buffer.construct_exemplar_set(self.model, x.cpu(), y.cpu(), task_id)\n",
    "                    \n",
    "                    global_step += 1\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    # Print progress every 10 batches\n",
    "                    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                        avg_loss = epoch_loss / batch_count\n",
    "                        print(f\"    Step {global_step}: Batch {batch_idx+1}/{len(train_loader)}, Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "                # Average epoch losses\n",
    "                avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0.0\n",
    "                task_losses.append(avg_epoch_loss)\n",
    "                \n",
    "                # Average loss components for this epoch\n",
    "                for key in epoch_loss_breakdown:\n",
    "                    epoch_loss_breakdown[key] /= batch_count if batch_count > 0 else 1\n",
    "                epoch_loss_components.append(epoch_loss_breakdown)\n",
    "                \n",
    "                print(f\"    ✅ Epoch {epoch+1} completed - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "                print(f\"       Loss Components: Task={epoch_loss_breakdown['task']:.4f}, \"\n",
    "                      f\"Reg={epoch_loss_breakdown['regularization']:.4f}, \"\n",
    "                      f\"Replay={epoch_loss_breakdown['replay']:.4f}\")\n",
    "\n",
    "            # End of epoch: update regularizer per task\n",
    "            if hasattr(self.regularizer, 'update_per_task'):\n",
    "                self.regularizer.update_per_task()\n",
    "            \n",
    "            # Detailed evaluation after each task\n",
    "            print(f\"  🔍 Evaluating Task {task_id+1}...\")\n",
    "            acc, eval_loss = self.evaluate_detailed(task_id)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.task_acc_history.append(acc)\n",
    "            self.task_ids.append(task_id)\n",
    "            self.time_steps.append(global_step)\n",
    "            self.time_acc_history.append(acc)\n",
    "            self.loss_history.extend(task_losses)\n",
    "            \n",
    "            # Store task-level loss components (average of all epochs)\n",
    "            avg_task_components = {}\n",
    "            for key in epoch_loss_components[0].keys():\n",
    "                avg_task_components[key] = sum(epoch[key] for epoch in epoch_loss_components) / len(epoch_loss_components)\n",
    "            self.task_loss_components.append(avg_task_components)\n",
    "            \n",
    "            # Evaluate on all previous tasks (for backward transfer analysis)\n",
    "            all_task_accs = []\n",
    "            for prev_task_id in range(task_id + 1):\n",
    "                prev_acc, _ = self.evaluate_detailed(prev_task_id)\n",
    "                all_task_accs.append(prev_acc)\n",
    "            \n",
    "            print(f\"  ✅ Task {task_id+1} Results:\")\n",
    "            print(f\"     Current Task Accuracy: {acc:.4f}\")\n",
    "            print(f\"     Current Task Eval Loss: {eval_loss:.4f}\")\n",
    "            print(f\"     Training Loss (final epoch): {task_losses[-1]:.4f}\")\n",
    "            print(f\"     All Tasks Accuracy: {[f'{a:.3f}' for a in all_task_accs]}\")\n",
    "            print(f\"     Average Accuracy So Far: {sum(all_task_accs)/len(all_task_accs):.4f}\")\n",
    "            \n",
    "            if self.evaluating_result:\n",
    "                backward_transfer = sum(all_task_accs[:-1]) / max(1, len(all_task_accs)-1) if len(all_task_accs) > 1 else 0.0\n",
    "                print(f\"     Backward Transfer: {backward_transfer:.4f}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        # Final comprehensive evaluation\n",
    "        self.print_final_summary()\n",
    "        self.plot_results()\n",
    "\n",
    "    def print_final_summary(self):\n",
    "        \"\"\"Print comprehensive final summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎯 FINAL TRAINING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Total Tasks Completed: {self.n_tasks}\")\n",
    "        print(f\"Total Training Steps: {self.time_steps[-1] if self.time_steps else 0}\")\n",
    "        \n",
    "        # Final evaluation on all tasks\n",
    "        final_accs = []\n",
    "        for task_id in range(self.n_tasks):\n",
    "            acc, loss = self.evaluate_detailed(task_id)\n",
    "            final_accs.append(acc)\n",
    "        \n",
    "        avg_acc = sum(final_accs) / len(final_accs)\n",
    "        print(f\"\\n📊 Final Performance:\")\n",
    "        print(f\"Average Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"Individual Task Accuracies: {[f'{a:.4f}' for a in final_accs]}\")\n",
    "        print(f\"Best Task Performance: {max(final_accs):.4f}\")\n",
    "        print(f\"Worst Task Performance: {min(final_accs):.4f}\")\n",
    "        \n",
    "        # Loss analysis\n",
    "        if self.task_loss_components:\n",
    "            print(f\"\\n📈 Loss Component Analysis:\")\n",
    "            for i, components in enumerate(self.task_loss_components):\n",
    "                print(f\"Task {i+1}: Total={components['total']:.4f}, \"\n",
    "                      f\"Task={components['task']:.4f}, \"\n",
    "                      f\"Reg={components['regularization']:.4f}, \"\n",
    "                      f\"Replay={components['replay']:.4f}\")\n",
    "\n",
    "    def plot_results(self):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Subplot 1: Accuracy over time\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(self.time_steps, self.time_acc_history, marker='o', label='Accuracy over time')\n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Online CL: Accuracy over Time')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Subplot 2: Accuracy per task\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(self.task_ids, self.task_acc_history, marker='s', color='orange', label='Accuracy per Task')\n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Online CL: Accuracy per Task')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Subplot 3: Loss components over tasks\n",
    "        if self.task_loss_components:\n",
    "            plt.subplot(2, 3, 3)\n",
    "            tasks = list(range(len(self.task_loss_components)))\n",
    "            total_losses = [comp['total'] for comp in self.task_loss_components]\n",
    "            task_losses = [comp['task'] for comp in self.task_loss_components]\n",
    "            reg_losses = [comp['regularization'] for comp in self.task_loss_components]\n",
    "            \n",
    "            plt.plot(tasks, total_losses, marker='o', label='Total Loss')\n",
    "            plt.plot(tasks, task_losses, marker='s', label='Task Loss')\n",
    "            plt.plot(tasks, reg_losses, marker='^', label='Regularization Loss')\n",
    "            plt.xlabel('Task')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss Components per Task')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        # Subplot 4: Final accuracy matrix (heatmap-style)\n",
    "        plt.subplot(2, 3, 4)\n",
    "        final_accs = []\n",
    "        for task_id in range(self.n_tasks):\n",
    "            acc, _ = self.evaluate_detailed(task_id)\n",
    "            final_accs.append(acc)\n",
    "        \n",
    "        plt.bar(range(self.n_tasks), final_accs, color='skyblue', alpha=0.7)\n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Final Accuracy')\n",
    "        plt.title('Final Accuracy per Task')\n",
    "        plt.xticks(range(self.n_tasks), [f'T{i+1}' for i in range(self.n_tasks)])\n",
    "        plt.grid(True, axis='y')\n",
    "        \n",
    "        # Subplot 5: Training loss over time\n",
    "        if self.loss_history:\n",
    "            plt.subplot(2, 3, 5)\n",
    "            plt.plot(self.loss_history, marker='.', alpha=0.7)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Training Loss')\n",
    "            plt.title('Training Loss over Time')\n",
    "            plt.grid(True)\n",
    "        \n",
    "        # Subplot 6: Memory usage (if available)\n",
    "        plt.subplot(2, 3, 6)\n",
    "        memory_usage = []\n",
    "        for task_id in range(self.n_tasks):\n",
    "            if hasattr(self.replay_buffer, 'get_all_data'):\n",
    "                data = self.replay_buffer.get_all_data()\n",
    "                if data[0] is not None:\n",
    "                    memory_usage.append(len(data[0]))\n",
    "                else:\n",
    "                    memory_usage.append(0)\n",
    "            else:\n",
    "                memory_usage.append(0)\n",
    "        \n",
    "        plt.plot(range(len(memory_usage)), memory_usage, marker='o', color='red')\n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Replay Buffer Size')\n",
    "        plt.title('Memory Usage over Tasks')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11733b69",
   "metadata": {},
   "source": [
    "# Offline Continual Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "819ecb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class OfflineCLTrainer:\n",
    "    \"\"\"\n",
    "    Orchestrates Offline Continual Learning training and evaluation using the best modules from above.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader_fn,  # function(task_id) -> DataLoader\n",
    "        test_loader_fn,   # function(task_id) -> DataLoader\n",
    "        device,\n",
    "        n_tasks,\n",
    "        n_classes,\n",
    "        evaluating_result=True,\n",
    "        epochs_per_task=10,\n",
    "        lr=0.01,\n",
    "        buffer_size=2000,\n",
    "        max_mem_size=4000,\n",
    "        regularizer_type='EWC'\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.n_tasks = n_tasks\n",
    "        self.n_classes = n_classes\n",
    "        self.evaluating_result = evaluating_result\n",
    "        self.epochs_per_task = epochs_per_task\n",
    "        self.lr = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.max_mem_size = max_mem_size\n",
    "\n",
    "        # Replay Buffer: iCaRL (best for offline CL)\n",
    "        self.replay_buffer = iCaRLReplayBuffer(max_mem_size=max_mem_size, device=device)\n",
    "\n",
    "        # OOD Detector: Energy-based\n",
    "        self.ood_detector = EnergyDetector()\n",
    "\n",
    "        # Task Boundary Detector: Scheduled (offline, so schedule is known)\n",
    "        self.task_boundary_detector = ScheduledTaskBoundaryDetector(\n",
    "            switch_interval=buffer_size, buffer_size=buffer_size\n",
    "        )\n",
    "\n",
    "        # Regularization: EWC or SI\n",
    "        if regularizer_type == 'EWC':\n",
    "            self.regularizer = EWC(self.model, device, ewc_lambda=1.0)\n",
    "        else:\n",
    "            self.regularizer = SynapticIntelligence(self.model, si_lambda=0.1)\n",
    "\n",
    "        # Loss Function\n",
    "        self.loss_fn = ContinualLearningLoss(\n",
    "            task_criterion=torch.nn.CrossEntropyLoss(),\n",
    "            regularizers=[self.regularizer],\n",
    "            replay_buffer=self.replay_buffer,\n",
    "            ood_detector=self.ood_detector,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Data loader functions\n",
    "        self.train_loader_fn = train_loader_fn\n",
    "        self.test_loader_fn = test_loader_fn\n",
    "\n",
    "        # Enhanced tracking\n",
    "        self.task_acc_history = []\n",
    "        self.time_acc_history = []\n",
    "        self.task_ids = []\n",
    "        self.time_steps = []\n",
    "        self.loss_history = []\n",
    "        self.task_loss_components = []\n",
    "\n",
    "    def evaluate_detailed(self, task_id):\n",
    "        \"\"\"Enhanced evaluation with detailed metrics\"\"\"\n",
    "        self.model.eval()\n",
    "        loader = self.test_loader_fn(task_id)\n",
    "        correct, total = 0, 0\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                logits, _ = self.model(x)\n",
    "                \n",
    "                # Accuracy calculation\n",
    "                pred = logits.argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "                \n",
    "                # Loss calculation\n",
    "                loss_components = self.loss_fn(self.model, x, y, logits, return_components=True)\n",
    "                total_loss += loss_components['total_loss'].item()\n",
    "                batch_count += 1\n",
    "        \n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0.0\n",
    "        \n",
    "        return acc, avg_loss\n",
    "\n",
    "    def evaluate(self, task_id):\n",
    "        \"\"\"Simple evaluation for backward compatibility\"\"\"\n",
    "        acc, _ = self.evaluate_detailed(task_id)\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        print(f\"Starting Offline Continual Learning with {self.n_tasks} tasks\")\n",
    "        print(f\"Model: {self.model.__class__.__name__}\")\n",
    "        print(f\"Regularizer: {self.regularizer.__class__.__name__}\")\n",
    "        print(f\"Replay Buffer: {self.replay_buffer.__class__.__name__}\")\n",
    "        print(f\"Epochs per task: {self.epochs_per_task}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        global_step = 0\n",
    "        for task_id in range(self.n_tasks):\n",
    "            print(f\"\\n🔄 Starting Task {task_id+1}/{self.n_tasks}\")\n",
    "            train_loader = self.train_loader_fn(task_id)\n",
    "            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n",
    "            \n",
    "            task_losses = []\n",
    "            epoch_loss_components = []\n",
    "            \n",
    "            for epoch in range(self.epochs_per_task):\n",
    "                epoch_loss = 0.0\n",
    "                batch_count = 0\n",
    "                epoch_loss_breakdown = {'total': 0, 'task': 0, 'regularization': 0, 'replay': 0, 'ood': 0, 'distillation': 0}\n",
    "                \n",
    "                print(f\"  📊 Epoch {epoch+1}/{self.epochs_per_task}\")\n",
    "                \n",
    "                for batch_idx, (x, y) in enumerate(train_loader):\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "                    self.model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    logits, _ = self.model(x)\n",
    "                    loss_components = self.loss_fn(self.model, x, y, logits, return_components=True)\n",
    "                    loss = loss_components['total_loss']\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Track losses\n",
    "                    epoch_loss += loss.item()\n",
    "                    for key in epoch_loss_breakdown:\n",
    "                        if key == 'total':\n",
    "                            epoch_loss_breakdown[key] += loss.item()\n",
    "                        else:\n",
    "                            component_key = f'{key}_loss'\n",
    "                            if component_key in loss_components:\n",
    "                                epoch_loss_breakdown[key] += loss_components[component_key].item()\n",
    "                    \n",
    "                    # Update regularizer\n",
    "                    if hasattr(self.regularizer, 'update_in_training'):\n",
    "                        self.regularizer.update_in_training()\n",
    "                    \n",
    "                    # Update replay buffer\n",
    "                    self.replay_buffer.construct_exemplar_set(self.model, x.cpu(), y.cpu(), task_id)\n",
    "                    \n",
    "                    global_step += 1\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    # Print progress every 20 batches (less frequent for offline)\n",
    "                    if (batch_idx + 1) % 20 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                        avg_loss = epoch_loss / batch_count\n",
    "                        print(f\"    Step {global_step}: Batch {batch_idx+1}/{len(train_loader)}, Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "                # Average epoch losses\n",
    "                avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0.0\n",
    "                task_losses.append(avg_epoch_loss)\n",
    "                \n",
    "                # Average loss components for this epoch\n",
    "                for key in epoch_loss_breakdown:\n",
    "                    epoch_loss_breakdown[key] /= batch_count if batch_count > 0 else 1\n",
    "                epoch_loss_components.append(epoch_loss_breakdown)\n",
    "                \n",
    "                print(f\"    ✅ Epoch {epoch+1} completed - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "                print(f\"       Loss Components: Task={epoch_loss_breakdown['task']:.4f}, \"\n",
    "                      f\"Reg={epoch_loss_breakdown['regularization']:.4f}, \"\n",
    "                      f\"Replay={epoch_loss_breakdown['replay']:.4f}\")\n",
    "\n",
    "            # End of task: update regularizer per task\n",
    "            if hasattr(self.regularizer, 'update_per_task'):\n",
    "                if isinstance(self.regularizer, EWC):\n",
    "                    self.regularizer.update_per_task(train_loader, torch.nn.CrossEntropyLoss())\n",
    "                else:\n",
    "                    self.regularizer.update_per_task()\n",
    "            \n",
    "            # Detailed evaluation after each task\n",
    "            print(f\"  🔍 Evaluating Task {task_id+1}...\")\n",
    "            acc, eval_loss = self.evaluate_detailed(task_id)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.task_acc_history.append(acc)\n",
    "            self.task_ids.append(task_id)\n",
    "            self.time_steps.append(global_step)\n",
    "            self.time_acc_history.append(acc)\n",
    "            self.loss_history.extend(task_losses)\n",
    "            \n",
    "            # Store task-level loss components\n",
    "            avg_task_components = {}\n",
    "            for key in epoch_loss_components[0].keys():\n",
    "                avg_task_components[key] = sum(epoch[key] for epoch in epoch_loss_components) / len(epoch_loss_components)\n",
    "            self.task_loss_components.append(avg_task_components)\n",
    "            \n",
    "            # Evaluate on all previous tasks\n",
    "            all_task_accs = []\n",
    "            for prev_task_id in range(task_id + 1):\n",
    "                prev_acc, _ = self.evaluate_detailed(prev_task_id)\n",
    "                all_task_accs.append(prev_acc)\n",
    "            \n",
    "            print(f\"  ✅ Task {task_id+1} Results:\")\n",
    "            print(f\"     Current Task Accuracy: {acc:.4f}\")\n",
    "            print(f\"     Current Task Eval Loss: {eval_loss:.4f}\")\n",
    "            print(f\"     Training Loss (final epoch): {task_losses[-1]:.4f}\")\n",
    "            print(f\"     All Tasks Accuracy: {[f'{a:.3f}' for a in all_task_accs]}\")\n",
    "            print(f\"     Average Accuracy So Far: {sum(all_task_accs)/len(all_task_accs):.4f}\")\n",
    "            \n",
    "            if self.evaluating_result:\n",
    "                backward_transfer = sum(all_task_accs[:-1]) / max(1, len(all_task_accs)-1) if len(all_task_accs) > 1 else 0.0\n",
    "                print(f\"     Backward Transfer: {backward_transfer:.4f}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        # Final comprehensive evaluation\n",
    "        self.print_final_summary()\n",
    "        self.plot_results()\n",
    "\n",
    "    def print_final_summary(self):\n",
    "        \"\"\"Print comprehensive final summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎯 FINAL TRAINING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Total Tasks Completed: {self.n_tasks}\")\n",
    "        print(f\"Total Training Steps: {self.time_steps[-1] if self.time_steps else 0}\")\n",
    "        print(f\"Total Epochs: {self.n_tasks * self.epochs_per_task}\")\n",
    "        \n",
    "        # Final evaluation on all tasks\n",
    "        final_accs = []\n",
    "        for task_id in range(self.n_tasks):\n",
    "            acc, loss = self.evaluate_detailed(task_id)\n",
    "            final_accs.append(acc)\n",
    "        \n",
    "        avg_acc = sum(final_accs) / len(final_accs)\n",
    "        print(f\"\\n📊 Final Performance:\")\n",
    "        print(f\"Average Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"Individual Task Accuracies: {[f'{a:.4f}' for a in final_accs]}\")\n",
    "        print(f\"Best Task Performance: {max(final_accs):.4f}\")\n",
    "        print(f\"Worst Task Performance: {min(final_accs):.4f}\")\n",
    "        \n",
    "        # Catastrophic forgetting analysis\n",
    "        initial_acc = self.task_acc_history[0] if self.task_acc_history else 0.0\n",
    "        final_first_task_acc = final_accs[0] if final_accs else 0.0\n",
    "        forgetting = initial_acc - final_first_task_acc\n",
    "        print(f\"Catastrophic Forgetting (Task 1): {forgetting:.4f}\")\n",
    "        \n",
    "        # Loss analysis\n",
    "        if self.task_loss_components:\n",
    "            print(f\"\\n📈 Loss Component Analysis:\")\n",
    "            for i, components in enumerate(self.task_loss_components):\n",
    "                print(f\"Task {i+1}: Total={components['total']:.4f}, \"\n",
    "                      f\"Task={components['task']:.4f}, \"\n",
    "                      f\"Reg={components['regularization']:.4f}, \"\n",
    "                      f\"Replay={components['replay']:.4f}\")\n",
    "\n",
    "    def plot_results(self):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Same plotting structure as OnlineCLTrainer but adapted for offline learning\n",
    "        # Subplot 1: Accuracy over time\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(self.time_steps, self.time_acc_history, marker='o', label='Accuracy over time')\n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Offline CL: Accuracy over Time')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Subplot 2: Accuracy per task\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(self.task_ids, self.task_acc_history, marker='s', color='orange', label='Accuracy per Task')\n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Offline CL: Accuracy per Task')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Subplot 3: Loss components over tasks\n",
    "        if self.task_loss_components:\n",
    "            plt.subplot(2, 3, 3)\n",
    "            tasks = list(range(len(self.task_loss_components)))\n",
    "            total_losses = [comp['total'] for comp in self.task_loss_components]\n",
    "            task_losses = [comp['task'] for comp in self.task_loss_components]\n",
    "            reg_losses = [comp['regularization'] for comp in self.task_loss_components]\n",
    "            \n",
    "            plt.plot(tasks, total_losses, marker='o', label='Total Loss')\n",
    "            plt.plot(tasks, task_losses, marker='s', label='Task Loss')\n",
    "            plt.plot(tasks, reg_losses, marker='^', label='Regularization Loss')\n",
    "            plt.xlabel('Task')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss Components per Task')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        # Subplot 4: Final accuracy matrix\n",
    "        plt.subplot(2, 3, 4)\n",
    "        final_accs = []\n",
    "        for task_id in range(self.n_tasks):\n",
    "            acc, _ = self.evaluate_detailed(task_id)\n",
    "            final_accs.append(acc)\n",
    "        \n",
    "        plt.bar(range(self.n_tasks), final_accs, color='lightcoral', alpha=0.7)\n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Final Accuracy')\n",
    "        plt.title('Final Accuracy per Task')\n",
    "        plt.xticks(range(self.n_tasks), [f'T{i+1}' for i in range(self.n_tasks)])\n",
    "        plt.grid(True, axis='y')\n",
    "        \n",
    "        # Subplot 5: Training loss over epochs\n",
    "        if self.loss_history:\n",
    "            plt.subplot(2, 3, 5)\n",
    "            plt.plot(self.loss_history, marker='.', alpha=0.7)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Training Loss')\n",
    "            plt.title('Training Loss over Epochs')\n",
    "            plt.grid(True)\n",
    "        \n",
    "        # Subplot 6: Forgetting analysis\n",
    "        plt.subplot(2, 3, 6)\n",
    "        if len(self.task_acc_history) > 1:\n",
    "            forgetting_rates = []\n",
    "            for i in range(len(self.task_acc_history)):\n",
    "                current_task_final_acc, _ = self.evaluate_detailed(i)\n",
    "                initial_acc = self.task_acc_history[i]\n",
    "                forgetting = initial_acc - current_task_final_acc\n",
    "                forgetting_rates.append(forgetting)\n",
    "            \n",
    "            plt.bar(range(len(forgetting_rates)), forgetting_rates, color='red', alpha=0.6)\n",
    "            plt.xlabel('Task')\n",
    "            plt.ylabel('Forgetting Rate')\n",
    "            plt.title('Catastrophic Forgetting per Task')\n",
    "            plt.xticks(range(len(forgetting_rates)), [f'T{i+1}' for i in range(len(forgetting_rates))])\n",
    "            plt.grid(True, axis='y')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9084243",
   "metadata": {},
   "source": [
    "# Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1224dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage pipeline for selecting Online or Offline Continual Learning\n",
    "\n",
    "def run_cl_pipeline(\n",
    "    cl_type,  # \"online\" or \"offline\"\n",
    "    model,\n",
    "    train_loader_fn,\n",
    "    test_loader_fn,\n",
    "    device,\n",
    "    n_tasks,\n",
    "    n_classes,\n",
    "    evaluating_result=True,\n",
    "    epochs_per_task=1,\n",
    "    lr=0.01,\n",
    "    buffer_size=1000,\n",
    "    max_mem_size=2000,\n",
    "    regularizer_type='EWC'\n",
    "):\n",
    "    if cl_type.lower() == \"online\":\n",
    "        trainer = OnlineCLTrainer(\n",
    "            model=model,\n",
    "            train_loader_fn=train_loader_fn,\n",
    "            test_loader_fn=test_loader_fn,\n",
    "            device=device,\n",
    "            n_tasks=n_tasks,\n",
    "            n_classes=n_classes,\n",
    "            evaluating_result=evaluating_result,\n",
    "            epochs_per_task=epochs_per_task,\n",
    "            lr=lr,\n",
    "            buffer_size=buffer_size,\n",
    "            max_mem_size=max_mem_size,\n",
    "            regularizer_type=regularizer_type\n",
    "        )\n",
    "    elif cl_type.lower() == \"offline\":\n",
    "        trainer = OfflineCLTrainer(\n",
    "            model=model,\n",
    "            train_loader_fn=train_loader_fn,\n",
    "            test_loader_fn=test_loader_fn,\n",
    "            device=device,\n",
    "            n_tasks=n_tasks,\n",
    "            n_classes=n_classes,\n",
    "            evaluating_result=evaluating_result,\n",
    "            epochs_per_task=epochs_per_task,\n",
    "            lr=lr,\n",
    "            buffer_size=buffer_size,\n",
    "            max_mem_size=max_mem_size,\n",
    "            regularizer_type='EWC' if regularizer_type == 'OnlineEWC' else regularizer_type\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"cl_type must be either 'online' or 'offline'.\")\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "786cc606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [09:09<00:00, 310kB/s]  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Use ImprovedTaskSplitter for train to get class assignments\u001b[39;00m\n\u001b[32m     20\u001b[39m splitter = ImprovedTaskSplitter(strategy=\u001b[33m'\u001b[39m\u001b[33madaptive_clustering\u001b[39m\u001b[33m'\u001b[39m, random_seed=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m train_loader_fn, _, task_class_assignments = \u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_task_loaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle_tasks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# For test dataset, use the same task_class_assignments to ensure consistent class splits\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Get class to indices for test\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(test_dataset, \u001b[33m'\u001b[39m\u001b[33mtargets\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 211\u001b[39m, in \u001b[36mImprovedTaskSplitter.create_task_loaders\u001b[39m\u001b[34m(self, base_dataset, n_tasks, batch_size, model, device, shuffle_tasks)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Apply splitting strategy\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strategy == \u001b[33m'\u001b[39m\u001b[33madaptive_clustering\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     class_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_class_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     task_class_assignments = \u001b[38;5;28mself\u001b[39m.adaptive_clustering_split(classes, class_features, n_tasks)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strategy == \u001b[33m'\u001b[39m\u001b[33msimilarity_based\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mImprovedTaskSplitter.extract_class_features\u001b[39m\u001b[34m(self, base_dataset, model, device, batch_size)\u001b[39m\n\u001b[32m     55\u001b[39m     batch_data = torch.stack([base_dataset[idx][\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch_indices])\n\u001b[32m     56\u001b[39m     batch_data = batch_data.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     _, features = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming model returns (logits, features)\u001b[39;00m\n\u001b[32m     59\u001b[39m     features_list.append(features.cpu())\n\u001b[32m     61\u001b[39m all_features = torch.cat(features_list, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mWideResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.block1(out)\n\u001b[32m     98\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.block2(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = WideResNet(depth=28, n_classes=10, widen_factor=2, dropout=0.3)\n",
    "\n",
    "# Create dataset loaders (example with CIFAR-10)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                           download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                          download=True, transform=transform)\n",
    "\n",
    "# Use ImprovedTaskSplitter for train to get class assignments\n",
    "splitter = ImprovedTaskSplitter(strategy='adaptive_clustering', random_seed=42)\n",
    "train_loader_fn, _, task_class_assignments = splitter.create_task_loaders(\n",
    "    base_dataset=train_dataset,\n",
    "    n_tasks=5,\n",
    "    batch_size=64,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    shuffle_tasks=True\n",
    ")\n",
    "\n",
    "# For test dataset, use the same task_class_assignments to ensure consistent class splits\n",
    "# Get class to indices for test\n",
    "if hasattr(test_dataset, 'targets'):\n",
    "    test_labels = test_dataset.targets\n",
    "else:\n",
    "    test_labels = [test_dataset[i][1] for i in range(len(test_dataset))]\n",
    "\n",
    "test_classes = sorted(set(test_labels))\n",
    "test_class_to_indices = {cls: [] for cls in test_classes}\n",
    "for idx, label in enumerate(test_labels):\n",
    "    test_class_to_indices[label].append(idx)\n",
    "\n",
    "# Create test task splits\n",
    "test_task_splits = []\n",
    "for task_classes in task_class_assignments:\n",
    "    indices = []\n",
    "    for cls in task_classes:\n",
    "        indices.extend(test_class_to_indices.get(cls, []))\n",
    "    random.shuffle(indices)  # Shuffle if desired\n",
    "    test_task_splits.append(indices)\n",
    "\n",
    "# Custom TaskDataset (already defined in the notebook)\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, base_dataset, indices):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.indices = indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.base_dataset[self.indices[idx]]\n",
    "\n",
    "# Define test_loader_fn\n",
    "def test_loader_fn(task_id):\n",
    "    dataset = TaskDataset(test_dataset, test_task_splits[task_id])\n",
    "    return DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Now run the pipeline with the improved loaders\n",
    "trainer = run_cl_pipeline(\n",
    "    cl_type=\"offline\",\n",
    "    model=model,\n",
    "    train_loader_fn=train_loader_fn,\n",
    "    test_loader_fn=test_loader_fn,\n",
    "    device=device,\n",
    "    n_tasks=5,\n",
    "    n_classes=10,\n",
    "    evaluating_result=True,\n",
    "    epochs_per_task=1,\n",
    "    lr=0.01,\n",
    "    buffer_size=1000,\n",
    "    max_mem_size=2000,\n",
    "    regularizer_type='EWC'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
